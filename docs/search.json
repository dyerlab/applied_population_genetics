[
  {
    "objectID": "about_the_author.html",
    "href": "about_the_author.html",
    "title": "About the Author",
    "section": "",
    "text": "Rodney Dyer is an Professor at Virginia Commonwealth University. When not writing code and developing new analytical approaches for understanding the spatial distribution of genetic variation, he enjoys fishing, fermentation sciences, and spending time with his family outdoors.",
    "crumbs": [
      "About the Author"
    ]
  },
  {
    "objectID": "r_ecosystem.html",
    "href": "r_ecosystem.html",
    "title": "\n1  The R Ecosystem\n",
    "section": "",
    "text": "1.1 Getting R Configured\nThe grammar of the R language was derived from another system called S-Plus. S-Plus was a proprietary analysis platform developed by AT&T and licenses were sold for its use, mostly in industry and education. Given the closed nature of the S-Plus platform, R was developed with the goal of creating an interpreter that could read grammar similar to S-Plus but be available to the larger research community. The use of R has increased dramatically due to its open nature and the ability of people to share code solutions with relatively little barriers.\nThe main repository for R is located at the CRAN Repository, which is where you can download the latest version. It is in your best interests to make sure you update the underlying R system, changes are made continually (perhaps despite the outward appearance of the website).\nknitr::include_graphics(\"./media/CRAN.png\")\nThe current version of this book uses version 3.2. To get the correct version, open the page and there should be a link at the top for your particular computing platform. Download the appropriate version and install it following the instructions appropriate for your computer.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The R Ecosystem</span>"
    ]
  },
  {
    "objectID": "r_ecosystem.html#getting-r-configured",
    "href": "r_ecosystem.html#getting-r-configured",
    "title": "\n1  The R Ecosystem\n",
    "section": "",
    "text": "1.1.1 Packages\nThe base R system comes with enough functionality to get you going. By default, there is only a limited amount of functionality in R, which is a great thing. You only load and use the packages that you intend to use. There are just too many packages to have them all loaded into memory at all times and there is such a broad range of packages, it is not likely you’d need more than a small fraction of the packages during the course of all your analyses. Once you have a package, you can tell R that you intend to use it by either\n\nlibrary(package_name)\n\nor\n\n library(package_name)\n\nThey are approximately equivalent, differing in only that the second one actually returns a TRUE or FALSE indicating the presence of that library on you machine. If you do not want to be mocked by other users of R, I would recommend the library version—there are situations where require will not do what you think you want it to do (even though it is a verb and should probably be the correct one to use grammatically).\nThere are, at present, a few different places you can get packages. The packages can either be downloaded from these online repositories and installed into R or you can install them from within R itself. Again, I’ll prefer the latter as it is a bit more straightforward.\n\n1.1.2 CRAN\nThe main repository for packages is hosted by the r-project page itself. There are packages with solutions to analyses ranging from Approximate Bayesian Computation to Zhang & Pilon’s approaches to characterizing climatic trends. The list of these packages is large and ever growing. It can be found on CRAN under the packages menu. To install a package from this repository, you use the function\n\ninstall.packages(\"thePackageName\") \n\nYou can see that R went to the CRAN mirror site (I use the rstudio one), downloaded the package, look for particular dependencies that that package may have and download them as well for you. It should install these packages for you and give you an affirmative message indicating it had done so.\nAt times, there are some packages that are not available in binary form for all computer systems (the rgdal package is one that comes to mind for which we will provide a work around later) and the packages need to be compiled. This means that you need to have some additional tools and/or libraries on your machine to take the code, compile it, and link it together to make the package. In these cases, the internet and the documentation that the developer provide are key to your sanity.\n\n1.1.3 GitHub\nThere are an increasing number of projects that are being developed either in the open source community or shared among a set of developers. These projects are often hosted on http://www.github.com where you can get access to the latest code updates. The packages that I develop are hosted on Github at (http://github.com/dyerlab) and only pushed to CRAN when I make major changes.\n\nknitr::include_graphics(\"./media/DyerlabGithub.png\")\n\n\n\n\n\n\n\nTo install packages from Github you need to install the devtools library from CRAN first\n\ninstall.packages(\"devtools\")\nlibrary(devtools)\n\nThen you need to use the devtools function install_github() to go grab it. To do so you need two separate pieces of information, the name of the developer who is creating the repository and the name of the repository it is contained within. For the gstudio package, the develop is dyerlab and the repository name is gstudio. If you are comfortable with git, you can also check out certain branches of development if you like with optional arguments to the install_github() function. Here is how you would install both the gstudio and popgraph libraries I use in this book.\n\ninstall_github(\"dyerlab/gstudio\")\ninstall_github(\"dyerlab/popgraph\")\n\nIf you are starting to work with R and intend to compile packages, there are some tools that you will need to have on your machine. For Windows machines, there is an rtools download EXE that you can get. On OSX, you need to download the developers tools from Apple (available on the Application Store for free). In either case, the developer (if they care about their code being used by others) should provide sufficient documentation. If you are working on Windows, I do not know how that system (and never have used it for any prolonged period of time) so you’ll have to look to the internet on where to find compilers and other developer tools.\n\n1.1.4 Bioconductor\nThe last main location to find packages is on the Bioconductor site, a collection of software for bioinformatic analyses. To install from the bioconductor site you need to download their own installer as:\n\nsource(\"http://bioconductor.org/biocLite.R\")\nbiocLite()\n\nAnd then to install packages you use\n\nbiocLite(c(\"GenomicFeatures\", \"AnnotationDbi\"))\n\nThere are no libraries used in this text from bioconductor as this text is more focused on marker-based population genetics and not sequences, annotations, etc.\n\n1.1.5 Troublesome Packages\nSome packages provide a unique set of problems for getting them onto your computer. There are a variety of reasons for this and Google is your friend (though it would be easier if R was named something more unique than the 18^th^ letter of the alphabet…).\n\n1.1.5.1 RGDAL & RGEOS\nEvery time I upgrade in any significant way, two R libraries seem to raise their ugly heads and scream like a spoiled child—rgdal and rgeos. Why do these two have to be SOOOO much of a pain? Why can’t we have a auto build of a binary with all the options in it for OSX? Who knows? I always feel like I get the fuzzy end of the lollipop with these two. Here is my latest approach for getting them going.\nFirst you have to make sure you have the latest GDAL libraries. I used to get mine from Kyngchaos, just download the framework, install it, and then do some kind of long R CMD INSTALL dance, which seems to no longer work for me. I also tried installing from Ripley’s repository and found that (a) It was a version older than the one I already had on my machine, and (b) you can’t install from that repository, there is a malformed header and the install.packages() function just barfs.\nTime to try something new. I typically stay away from the various installer frameworks out there on OSX to keep everything in Frameworks. But this time, I used MacPorts. You can find the latest version here. Here is how I got it to help me out.\n\nDownload XCode from Apple, it is both free and has a lot of tools that make your life as a programmer easier. It is a big package and you’ll have to install the command line developer tools as well. You will be prompted on how to do this.\nDownloaded the version of macports for your OS, I’m currently on 10.11 and installed it with little problems. It takes a bit of time at the end of the installation because it is downloading a lot of information. Be patient.\nIn the terminal, I updated it sudo ports -v selfupdate and again be patient, it is going to grab a lot of stuff from the internet.\nI then used it to install gdal as a unix library (rather than as a framework so it won’t be located in /Library/Frameworks) by sudo ports install gdal. There were a lot of dependencies for this one so it took a while.\nI then had R install rgdal as install.packages( rgdal, type=\"source\")\n\n\nWorked like a charm.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The R Ecosystem</span>"
    ]
  },
  {
    "objectID": "r_ecosystem.html#libraries-used-in-text",
    "href": "r_ecosystem.html#libraries-used-in-text",
    "title": "\n1  The R Ecosystem\n",
    "section": "\n1.2 Libraries Used in Text",
    "text": "1.2 Libraries Used in Text\nThis work requires several libraries that you may need to get from either CRAN or GitHub. The following if you run the following code, you should be up-to-date on the necessary packages used throughout this text.\n\nfiles &lt;- list.files(\".\",pattern=\".rmd\")\nlibraries &lt;- c(\"gstudio\",\"popgraph\",\"ggplot2\")\nfor( file in files ) {\n  suppressWarnings(s &lt;- system( paste(\"grep -E -o 'library\\\\(\\\\w+\\\\)'\",file), intern=TRUE))\n  if( length(s) &gt; 0 ) {\n    s &lt;- strsplit(s,\"(\",fixed=TRUE)\n    for( item in s ){\n      if( length(item) == 2){\n        library &lt;- strsplit(item[2],\")\",fixed=TRUE)[[1]]\n        libraries &lt;- c(libraries,library)\n      }\n    }\n  }\n}\npkgs &lt;- sort( unique(libraries) )\nidx &lt;- which( pkgs == \"package_name\" )\npkgs &lt;- pkgs[-idx]\n\n\n\npkgs_df &lt;- data.frame(Name = pkgs, Title = NA)\nfor(i in seq_along(pkgs)){\n  f = system.file(package = pkgs[i], \"DESCRIPTION\")\n  if( nchar(f)&gt; 1) {\n    # Title is always on 3rd line\n    title = readLines(f)\n    title = title[grep(\"Title: \", title)]\n    pkgs_df$Title[i] = gsub(\"Title: \", \"\", title)    \n  }\n}\nknitr::kable(pkgs_df,caption = \"R packages used in the examples shown in this book.\")\n\n# make sure these libraries are already installed on this machine\ninst_pkgs &lt;- installed.packages()\nto_install &lt;- setdiff( pkgs, inst_pkgs )\nif( length(to_install))\n  install.packages(to_install, repos=\"https://cran.rstudio.org\")\n\n\n# See if maxent.jar is installed in the dismo java package.\npath &lt;- system.file(\"java\", package=\"dismo\")\nfiles &lt;- list.files(system.file(\"java\", package=\"dismo\"))\nif( !(\"maxent.jar\" %in% files) )\n  system( paste(\"cp ./spatial_data/maxent.jar\", path) )",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The R Ecosystem</span>"
    ]
  },
  {
    "objectID": "r_ecosystem.html#the-rstudio-environment",
    "href": "r_ecosystem.html#the-rstudio-environment",
    "title": "\n1  The R Ecosystem\n",
    "section": "\n1.3 The RStudio Environment",
    "text": "1.3 The RStudio Environment\nBy itself, R can be used in the terminal or as the basic interface the installer provides. While both of these methods are sufficient for working in R, they are less than optimal (IMHO). I believe one of the most important tools you can use is a good IDE as it helps you organize and work with your data in ways that focus more on the outcome and less on the implementation. I’ve spent a lot of time in both Emacs and Vim and while they may be tools for the geek elite, RStudio has made me much more productive in terms of output per unit time.\n\nknitr::include_graphics(\"./media/RStudioIDE.png\")\n\n\n\n\n\n\n\nThe RStudio IDE can be can be downloaded directly from (http://rstudio.org) and comes in varieties for desktops and servers. The packages are easy to install and contain all the instructions you need. If you run your own server, you can install it and do your analyses via a web interface that looks identical to the desktop client (in fact it is more similar than you perhaps know).\nThe interface has four panes, two of which (the source code editor and the terminal output) you will work with the most often. The other panes have information on the history of your project, plot output, packages, help, and a file browser. You can type in R commands in the terminal window and receive responses directly, just as normal. However, you can also type in your code into a script file (files ending in *.R) and run them directly. You can also create Markdown and LaTeX documents embedding your code in with the verbiage, which is evaluated any time you make a html, rtf, docx, or pdf output. All the code from R for this book was written in RMarkdown and parsed as an html document. If you are a serious user of R, you can use LaTeX or RMarkdown to make documents, slides, presentations, posters, etc. It is a versatile tool and one worth looking into, especially as it pertains to reproducible research (something we all need to strive for).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The R Ecosystem</span>"
    ]
  },
  {
    "objectID": "reproducable_research.html",
    "href": "reproducable_research.html",
    "title": "\n6  Reproducable Research\n",
    "section": "",
    "text": "6.1 Keeping Data on Google Drive\nAs a scientist, the integrity of your data is your #1 priority. In fact, I argue that your ability to write advertisements of your research, which is what manuscripts really are, is secondary to the data that you collect and provide the community. As a practitioner, you get more credit and are providing more to your research community if your data are not only collected with integrity but able to be used by others. In the near future, the DOI for your data sets will be as important as the ones for your manuscripts.\nFor your data to have the most impact, it needs to be both available and valid. One way to help keep your data intact is to put it in one place and only pull from there to do analyses when you need to do them. You should ALWAYS keep your data and your analyses as close to each other as possible. Once they get separated, things go crazy. For example, consider the case where you send your colleague a copy of your data in a spreadsheet. Now you have two potentially different (and divergent) representations of your data. Any change that either you or your colleague make to the data is not mirrored. Which one is correct? What happens when you put a copy on the laboratory computer and do some analyses with it, then copy it to your ‘backup drive’ (or even worse a thumb drive) and then put a copy of it on your laptop. Then when you get home, a copy goes on your home computer. How many different copies of your data are there in the world? Which one is correct? Somewhere down the road, you will need to recreate a figure for a manuscript, give someone a copy of your data, reanalyze your raw data with a new approach that you run across, etc. Which copy of the data that you haven’t worked on for the last 6+ months is the right one? Which one is valid? Even if you use an awesome naming scheme like “Data.last.version.xls”, “Data.really.last.version.xls”, “I.totally.mean.last.version.xls” etc.. Not going to serve you well in the long run. All of this is terrifying from the perspective of reproducibility.\nOne way to keep data available with integrity is to have it located in one, and only one, location. If you use Google Drive, this is a pretty easy thing to do. You can keep your spreadsheet online, and then just access the data for your analysis when you need to.\nHere is how you set that up. Upload your data into a Google Spreadsheet and then select File \\(\\to\\) Publish to the Web…\nYou will be given a dialog (as shown here) that allows you to determine how much of the spreadsheet to share. You can publish a particular sheet or the entire document. For our purposes, it is probably best to keep our data on a single sheet within the file and publish them independently. You can also determine the format you want the data presented in, at present the following are available:\nknitr::include_graphics(\"./media/GoogleShare.png\")\nIt is in my opinion preferable to keep your data in a format that is accessible to the broadest audience. It is not a secret that programs change file formats and old proprietary binary formats can become corrupted and unreadable. It would be a shame to keep the data that you work so hard on in a format that you can no longer access or that your potential future colleagues and collaborators cannot get access to…\nFor this example, the data from the first sheet are provided as a *.csv file and can be accessed by that long URL.\nIn R, we can access this spreadsheet directly through that URL, which allows us to have our data located in a single centralized location and accessible by a broad range of applications and uses, none of which have the ability to duplicate your data (thereby causing a fork) or make any changes to its internal structure. Here is how it is done.\nLoad in the gstudio library and read the data in as a normal data file. This last part is identical to what we would do for a normal file on our computer, it is the first two steps that extend this approach to use remote files via a url.\nlibrary(gstudio)\ncornus &lt;- read_population( \"data/Cornus.csv\", type=\"column\", locus.columns = 5:14, sep=\",\")\nsummary(cornus)\n\n   Population       SampleID      X.Coordinate   Y.Coordinate       Cf.G8    \n Min.   :2.000   Min.   :203.0   Min.   : 346   Min.   : 254   155:165 : 18  \n 1st Qu.:3.000   1st Qu.:315.5   1st Qu.:1482   1st Qu.:2231   165:165 : 15  \n Median :4.000   Median :428.0   Median :1656   Median :2928   167:167 : 13  \n Mean   :3.809   Mean   :428.0   Mean   :1747   Mean   :2588   155:159 : 12  \n 3rd Qu.:5.000   3rd Qu.:540.5   3rd Qu.:1914   3rd Qu.:3082   157:157 : 12  \n Max.   :6.000   Max.   :653.0   Max.   :3778   Max.   :6148   (Other) :372  \n                                                               NA's    :  9  \n      Cf.H18         Cf.N5          Cf.N10         Cf.O5    \n 105:119 : 23   170:170 :251   189:193 : 25   182:196 : 43  \n 105:105 : 18   162:170 : 34   189:201 : 20   182:182 : 41  \n 107:119 : 16   172:172 : 34   189:197 : 18   178:196 : 28  \n 121:121 : 16   164:170 : 27   189:189 : 17   178:182 : 25  \n 105:113 : 15   166:170 : 19   193:193 : 17   180:180 : 24  \n (Other) :362   (Other) : 50   (Other) :341   (Other) :282  \n NA's    :  1   NA's    : 36   NA's    : 13   NA's    :  8\nThe key here is that the contents of the data file are available but you cannot manipulate it. If you do need to make changes to the raw data, you can do this through the Google Drive interface. Once you do that, all you would have to do is rerun your scripts and every one of them will point to the only and latest version of the data set.\nI recommend looking into more approaches on Reproducible Research as they will help you, professionally, get the most out of your efforts.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducable Research</span>"
    ]
  },
  {
    "objectID": "reproducable_research.html#keeping-data-on-google-drive",
    "href": "reproducable_research.html#keeping-data-on-google-drive",
    "title": "\n6  Reproducable Research\n",
    "section": "",
    "text": "Web page - An html equivalent of the page\n\nComma-separated values (*.csv) - This is the preferred format for our purposes.\n\nTab-separated values (*.tsv) - Columns separated by tabs. This is also an option for our needs.\n\nPDF Document (*.pdf) - Portable Document Formats for general dissemination.\n\nMicrosoft Excel (*.xlsx) - Excel formats are great for excel but not openly available.\nOpenDocument Spreadsheet (*.ods) - Spreadsheet file format from OpenOffice.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducable Research</span>"
    ]
  },
  {
    "objectID": "reproducable_research.html#integrating-into-slack",
    "href": "reproducable_research.html#integrating-into-slack",
    "title": "\n6  Reproducable Research\n",
    "section": "\n6.2 Integrating into Slack",
    "text": "6.2 Integrating into Slack\nIf you and your group use Slack to help in productivity, you can post text and results directly from R into your Slack channels without the copy-paste, save file, upload, etc. routine. Here is how you set it up. This is still a bit in development and the latest version needs a little tweeking to work correctly (see subsection below).\nWhile this package is currently on CRAN, I recommend downloading the latest version from github. First things first, I recommend installing the latest version from the github repository.\n\nlibrary(devtools)\ninstall_github(\"hrbrmstr/slackr\")\n\n\n6.2.1 Fixing Problem with Version 1.4.1\nThere is a slight problem though. The current version of the slackr library has an error in it associated with (perhaps) a recent change in the Slack API that has not been fixed by the developer.\nFor me to get this to work, I had to compile the package myself after making the following change in one file. To fix it, do the following:\n\nDownload (or checkout) the repository from github at: https://github.com/hrbrmstr/slackr\n\nOpen the project in RStudio\n\nOpen the R file names slackr_utils.R\n\nIn the function named slackr_ims the last line (line 117) is something like dplyr::left_join( %some stuff% ). Replace this line with suppressWarnings( merge(users, ims, by.x=\"id\", by.y='user') )\n\nThe compile and install the package as:\n\n\nlibrary(devtools)\nload_all()\nbuild()\ninstall()\n\nIt should work just fine from then on. Hopefully, on the next time that this package is updated by the author, the left_join() problem will have been resolved. This issue had been marked as “resolved” in the github issues a while back but apparently not pushed to the repository.\n\n6.2.2 Configuration\nYou can configure it from the command line, but perhaps the easiest way to get it going is to set up a config file. By default, it will look for the file ~/.slackr This is a plain text file (actually it is a Debian Control File - DCF) and needs the following content\napi_token: xoxp-XXXXXXXXXXX-XXXXXXXXX-XXXXXXXXXX-XXXXXXXXX\nchannel: #r\nusername: rodney\nincoming_webhook_url: https://hooks.slack.com/services/XXXXXXXX/XXXXXXXX/XXXXXXXX\n\nYou need to get the api_token and the incoming_webhook_url from slack itself. Save that file.\n\n6.2.3 Sending Text and Graphical Ouput\nBoth textual output (e.g., the results of an analysis) or graphical output (such as ggplot objects) can be sent to your slack channels now. Here are some examples:\n\nlibrary(slackr)\nslackr_setup()\nslackr(\"This is an incoming piece of text from RStudio\")\n\nWhich results in the following in my #r slack channel:\n\nknitr::include_graphics(\"media/slack_text_output.png\")\n\n\n\n\n\n\n\nThere is also a provision for sending output graphics like ggplot objects. Here is an example of a barplot of heterozygosity across all loci in the included arapat dataset.\n\nlibrary(gstudio)\nlibrary(ggplot2)\ndata(arapat)\nhe &lt;- genetic_diversity(arapat,mode=\"He\")\np &lt;- ggplot( he, aes(x=Locus, y=He)) + geom_bar(stat=\"identity\") + theme_bw()\nggslack(p)\n\nWhich produces the following image in my channel\n\nknitr::include_graphics(\"media/slack_ggplot_output.png\")\n\n\n\n\n\n\n\nVery Cool! I can imagine a lot of situations where it would be helpful to have a script send a slack notification when it has finished (or crapped out on an error) in some long-running analyses. Now if we could just get slack to integrate with RStudio server version…\n\n6.2.4 Markdown\nThis entire book is written in RMarkdown, a text-based approach for mixing verbiage and analyses into a single document. There are several advantages to adopting an open appraoch such as this including:\n1. All the data and analyses are in the same place. The only reason you see a graph above is because the markdown document has the specific code in it to make that graphical output. It is not done in some other program and either imported or cut-and-paste into this document.\n2. It is in a text format. If you wish, you can see the entirety of this text on my github site.\nThis means I can take advantage of: - Version control. I can make branches, develop new parts, and merge them back in. - No need for those ad hoc naming schemes (last_version.docx, really_last_version.docx, Im_totally_lost_which_version_is_this.docx), it is all kept in sync. - Collaborations with others is easy, just like real programmers. 3. Analyses and figures can be re-run and changes in the output can be inserted into your documents automatically. 4. Markdown documents are the source and can be converted into HTML, PDF, DOCX, presentations, etc. All from the same source materials. This means that you work on one file and when you need to convert it into a finalized format, you can do so with ease.\nTo get started working with markdown, go read over the RMarkdown website and fire up RStudio. It has all the components built-in for you.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducable Research</span>"
    ]
  },
  {
    "objectID": "genetic_drift.html",
    "href": "genetic_drift.html",
    "title": "\n8  Genetic Drift\n",
    "section": "",
    "text": "8.1 Consequences of Genetic Drift\nGenetic drift is the stochastic process resulting from a reduction in the size of a population. If populations are infinite in size, both allele and genotype frequencies should remain constant through time. However, if populations are not infinite (reality knocking), then drift may act as a diversifying force. This chapter examines the process of drift how we can measure its consequences.\nGenetic drift is the process by which random selections of gametes produced during mating events result in stochastic changes in allele frequencies through time. This is not a terribly ground shattering idea, and our understanding of random chance provides some great insights into the underlying mechanisms operating in populations.\nAs an analogy, consider the simulation scenario where we have two alleles, \\(A\\) and \\(B\\), at a haploid locus (for simplicity).\nCollections are drawn at random using different sample sizes\nWe are implicitly assuming that the frequencies of these two alleles are identical (e.g., \\(p_A = p_B = 0.5\\)). We can see the effects of genetic drift by running this ‘simulation’ in which populations are randomly created selected and used to estimate the frequency of an allele and how sample size influences the stability of the allele frequency. If N does not influence the estimation of allele frequencies then all examples should yield an estimated allele frequency equal to \\(0.5\\) (as they truly are).\nTo do this, we set up the parameters in a data.frame\nand then go through data frame, each row in order, and pull out random collections from Alleles in sizes equal to sample_size. To do this, I’m going to use a for() loop. This is simply a function that repeats a set of code a specified number of times. In this case, I will need to take the first row, make a random sample of alleles given the value of df\\$N[1], find the frequency of an allele in that random sample, and then store the frequency back into the data.frame at df$FreqA[1]. I then need to go to the second row and do the same thing again. And then the third row. Then the fourth. etc.\nThe format of a for() loop is pretty easy. You define a sequence of values to use (in this case I will use the variable named row to contain the sequence of integers from 1:nrow(df)). Each time row is assigned a value, the code within the curly brackets ‘{}’ after the for() statement is executed and the value of row is constant during that iteration.\nHere, row starts out at 1, then the code inside the brackets is run with row=1. After it reaches the end of the loop, row is incremented by one and the code is then run with row=2. This is repeated until row = nrow(df) and then the looping is finished. If you type this in and do it, you have done your first stochastic (some would call it Monte Carlo) simulation! Congratulations. That wasn’t so hard.\nLets now look at the data.\nThe frequencies observed in each run are all over the place, basically covering the entire allowable range for an allele frequency. More importantly though lets look graphically at the allele frequencies using a boxplot where frequencies are grouped by sample size, df$N.\nThe plot of these data show an asymptotic reduction in the variation around the estimated allele frequency as you increase N.\nThis is to be expected, as we sample more individuals, we will become more confident that the parameters we estimate are closer to the real value. In statistics, this is the law of large numbers or the Central Limit Theorem. It is the same thing here. If there is a small population, each mating event may have the opportunity to have a wide fluctuation in gamete allele frequencies. All other things being equal, the frequencies will change each generation, an amount inversely proportional to the size of the population.\nSmall populations are not rare, some organisms exist only at small densities (e.g., top predators). Others may have undergone population bottlenecks, exist in marginal habitats at the edge of their tolerance range, or have a life-history that goes through periodic outbreaks.\nAlgebraically, we can determine the probability of observing a specific number of alleles in the next generation given random mating assumptions and a small sample size. The selection of diploid alleles follows a binomial expansion.\n[ P(N_A|N,p_A) = p_A{N_A}(1-p_A){2N-N_A} ]\nThis expansion has two parts:\nThe binomial coefficient, \\(\\frac{2N!}{N_A!(2N-N_A)!}\\), that determines the number of different ways we can get two alleles by random draw and have \\(N_A\\) samples of the \\(A\\) allele.\nThe probability, \\(p_A^{N_A}(1-p_A)^{2N-N_A}\\), of observing any each occurrence of the those samples based upon the frequency of the \\(A\\) allele, \\(p_A\\), in the population.\nAs an example, consider the idealized situation where we have \\(N=20\\) individuals, all of which are heterozygotes. That means that for a diploid locus, \\(N_A = N_B = 20\\). The probability of observing exactly 20 \\(A\\) alleles in the next generation can be calculated as:\nwhere the binomial coefficient is given by\nand the probability of each one of those is\nand the final probability associated with observing \\(N_A=20\\) after a single round of random mating is\nWhich is not so good, if you are interested in stability of allele frequencies. Only XXX% of the time would the allele frequency stay the same and XXXX% of the time allele frequencies would change. This change is not so much an evolutionary forces as it is a stochastic process associated with low sample sizes.\nSo if it is not that likely that the allele frequencies will stay the same, what does the entire distribution of expected allele occurrences look like? Lets set the variable N.A equal to a sequence of potential allele counts (from 0 to 2*N) and then estimate and plot the distribution.\nEven for the tails of this distribution, the probabilities are not zero (here are the first 10 entries)\nmeaning that it is possible to start with everyone as a heterozygote and after a single random mating event, the population may either be fixed for the \\(A\\) allele or have lost it altogether and only have aa homozygotes. The shape of this distribution is not only influenced by \\(N\\) but also by \\(p\\). Here is what it looks like if the frequency of the allele is \\(0.10\\) instead of \\(0.50\\).\nThe more skewed the allele frequency is away from \\(0.5\\), the more likely the population will be to become fixed for one allele or the other.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Genetic Drift</span>"
    ]
  },
  {
    "objectID": "genetic_drift.html#consequences-of-genetic-drift",
    "href": "genetic_drift.html#consequences-of-genetic-drift",
    "title": "\n8  Genetic Drift\n",
    "section": "",
    "text": "8.1.1 Time to Fixation\nIn fact, there is a well known relationship between the probability of fixation and the combination of both allele frequency and population size. Namely, the expected time to fixation, tfix (in generations), for a population of size Ne with two alleles (occurring p and q) is:\n[ t_{fix} = ]\nwhich for values of \\(N\\) for \\(p=c(0.01, 0.1, 0.25, 0.5)\\) are\n\nlibrary(ggplot2)\ndf &lt;- data.frame()\nN &lt;- 10^(1:5)\nfor( p in c(0.01, 0.1, 0.25, 0.5)){\n  t &lt;- (-4*N*(1-p)*log(1-p))/p\n  df &lt;- rbind( df, data.frame(p,N,t))\n}\ndf$p &lt;- factor( df$p )\nggplot( df, aes(N,t,color=p)) + geom_line() + xlab(\"Population Size (N)\") + ylab(\"Expected Time to Fixation (generations)\")\n\n\n\n\n\n\n\nThis parameter Ne can be quite deviant from the census size \\(N\\) depending upon several features of the organisms life history. We return to that later and discuss it in depth, for the time being, lets just assume it is a measure of the size of a population. That said, the stochastic selection of alleles due only to population size, can have significant effects on allele frequencies, available genetic diversity, and genotypic composition. In this simple two-allele system (often referred to as the Wright-Fisher model), if drift is the only feature that is influencing allele and genotype frequencies, the variance in allele frequencies through time has an expectation of:\n[ ^2_p = pq]\nwhich if examined for changes in Ne for fixed p=0.5\n\nt &lt;- 1:100\np &lt;- q &lt;- 0.5\nne &lt;- function( t, Ne, p, q) { return( p*q*( 1 - exp( -t/(2*Ne)))) }\ndf &lt;- data.frame( Generation=c(t,t,t),\n                  sigma_p= c( ne(t,10,p,q), ne(t,100,p,q), ne(t,1000,p,q) ),\n                  Ne=rep(c(\"10\",\"100\",\"1000\"), each=100) ) \nggplot( df, aes(x=Generation, y=sigma_p, color=Ne)) + geom_line() +\n  xlab(\"Generation (t)\") + ylab(expression(sigma[p]^2))\n\n\n\nExpected variance in allele frequencies through time for a Wright-Fisher model of genetic drift for three different effective population sizes.\n\n\n\nor changes in p for fixed Ne = 100 are\n\ndf &lt;- data.frame( Generation=c(t,t,t),\n                  sigma_p= c( ne(t,100,.1,.9), ne(t,100,.25,.75), ne(t,100,.5,.5) ),\n                  p=rep(c(\"p=0.10\",\"p=0.25\",\"p=0.50\"), each=100) ) \nggplot( df, aes(x=Generation, y=sigma_p, color=p)) + geom_line() +\n  xlab(\"Generation (t)\") + ylab(expression(sigma[p]^2))\n\n\n\nExpected variance in allele frequencies through time for a 2-allele Wright-Fisher model of genetic drift across different starting allele frequencies.\n\n\n\nThe important distinction here between these two graphs are that:\n\nFor different effective population sizes, the larger the population, the more stable the allele frequencies through time. For \\(N_e = 1000\\), the variance in allele frequencies is relatively small, compared to the other population sizes, even after 100 generations.\n\nAllele frequencies show the opposite effect, with low allele frequencies, the variance is less than that in populations with larger allele frequencies (maximizing when \\(p = \\frac{1}{\\ell}\\), where \\(\\ell\\) is the number of alleles). There is more genetic variance with a more even distribution of allele frequencies (something we will return to when we talk about Fishers Fundamental Theorem).\n\n8.1.2 Time to Allele Loss\nTime to fixation relates to the loss of all alleles, though in the data we often deal with, there is not a simple \\(p=q\\), two allele system. However, the models developed thus far, can give us an idea of the expected time to allele loss by rearranging the expectations a bit.\n[ t_{loss} = -(p) ]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Genetic Drift</span>"
    ]
  },
  {
    "objectID": "genetic_drift.html#mutation-drift",
    "href": "genetic_drift.html#mutation-drift",
    "title": "\n8  Genetic Drift\n",
    "section": "\n8.2 Mutation & Drift",
    "text": "8.2 Mutation & Drift\nIf mutation is also a factor, then the time to allele loss, tloss, is modified a bit. We will come back to discussions of mutation in a later chapter and get a bit more in-depth to its consequences. However, depending upon the mutation model, its effects are generally that it increases the rate of allele loss rather than decrease it. This is because a random mutation (in a system where there are more than two alleles) is more likely to make allele A become something else than make one of the potential large number of other alleles into an A.\nWith a non-zero mutation rate, \\(\\mu\\), the time to loss is approximated to be\n[ t_{loss} = ]\nif and only if \\(N_e\\mu\\) is very small (e.g., &lt;&lt; 1). However, if it is larger than that, the expectation can be approximated as:\n[ t_{loss} = ]\nwhere the \\(\\gamma\\) term is Euler’s constant (Masel et al. 2007).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Genetic Drift</span>"
    ]
  },
  {
    "objectID": "mapping_populations.html",
    "href": "mapping_populations.html",
    "title": "\n16  Mapping Populations\n",
    "section": "",
    "text": "16.1 Procedure\nAlmost every data set we will work on has a spatial context. Until relatively recently, this was largely ignored and at most given as a 2-dimensional projection of sampling locations on a black and white map in our manuscripts. Here we explore methodologies to provide more intuitive approaches for plotting the spatial location of our sampling sites.\nFirst, we need to load in the proper libraries. Each of the libraries are needed for:",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mapping Populations</span>"
    ]
  },
  {
    "objectID": "mapping_populations.html#procedure",
    "href": "mapping_populations.html#procedure",
    "title": "\n16  Mapping Populations\n",
    "section": "",
    "text": "gstudio - Contains the A. attenuatus data set and there are built-in routines that allow you to plot populations in various formats or export the data in a format for plotting in other systems (e.g., GoogleEarth, ArcGIS, etc).\n\nggmap - Allows the plotting of tiles from a googleMap.\n\nleaflet - Loads in the javascript leaflet library and allows the interactive visualization.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mapping Populations</span>"
    ]
  },
  {
    "objectID": "mapping_populations.html#plotting-population-locations",
    "href": "mapping_populations.html#plotting-population-locations",
    "title": "\n16  Mapping Populations\n",
    "section": "\n16.2 Plotting Population Locations",
    "text": "16.2 Plotting Population Locations\nWe will start by loading in the default data set for gstudio and using methods in that package for producing spatial plots.\n\n# load in the libraries and grab the data\nlibrary(gstudio)\ndata(arapat)\n\n# pull out the coordinates - Population is the default stratums\ncoords &lt;- strata_coordinates(arapat)\ncoords$Stratum &lt;- as.character( coords$Stratum )\ncoords[1:5,]\n\n   Stratum Longitude Latitude\n1       88 -114.2935 29.32541\n11       9 -113.9449 29.01457\n20      84 -113.6679 28.96651\n29     175 -113.4897 28.72796\n36     177 -113.9914 28.66056\n\n\nWe can extend this basic plotting by adding text to the plots indicating the population names (here I offset the coordinates by 0.1 & 0.12 degrees—a cheap trick but it works pretty well here).",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mapping Populations</span>"
    ]
  },
  {
    "objectID": "mapping_populations.html#population-allele-frequencies",
    "href": "mapping_populations.html#population-allele-frequencies",
    "title": "\n16  Mapping Populations\n",
    "section": "\n16.3 Population Allele Frequencies",
    "text": "16.3 Population Allele Frequencies\nIt is also possible to plot the data in a spatial context. Here is an example of how to mix ggplot() and ggmap() data and I’ll plot the locations as proportional in size to the allele frequency.\n\n# Break out allele frequencies for LTRS by population \nf &lt;- frequencies(arapat,loci=\"LTRS\",stratum=\"Population\")\n\n# lets only use the allele 01 \nf &lt;- f[ f$Allele==\"01\",]\n\n# here are what the data look like\nf[1:10,]\n\n   Stratum Locus Allele Frequency\n1      101  LTRS     01 0.2777778\n3      102  LTRS     01 0.3125000\n5       12  LTRS     01 0.8000000\n7      153  LTRS     01 0.1500000\n11     159  LTRS     01 0.8888889\n13     160  LTRS     01 0.8000000\n15     161  LTRS     01 0.9500000\n17     162  LTRS     01 0.9500000\n19     163  LTRS     01 0.1000000\n21     164  LTRS     01 0.3500000\n\n\nand then use those frequencies as image size.\n\n# merget the frequencies into the coordinate data.frame\ncoords &lt;- merge( coords, f[,c(1,4)])\n\nor for observed effective allele size\n\n# grab effect for LTRS locus\nae &lt;- genetic_diversity(arapat, stratum = \"Population\", mode=\"Ae\")\nae &lt;- ae[ ae$Locus==\"LTRS\",]\n\n# merge in the data\ncoords &lt;- merge( coords, ae[,c(1,3)])\n\nThere is also the option to make use of some pie charts. I know, pie charts suck and any statistician will tell you that they should probably not be used because they can be misleading, but here they are. For exploratory data analysis, they can be very insightful at times. Here is the frequency of alleles at the enolase locus in Araptus. Any spatial structuring catch your eye?\n\n# pies_on_map( arapat, stratum=\"Population\", locus=\"EN\")\n\nWhich will open a new browser window and produce a graph like the one below.\n\n\nNote the messages about the approximation. This is because the google maps API has an integer for zoom factor and at times it is not able to get all the points into the field of view using an integer zoom. If this happens to you, you can manually specify the zoom as an optional argument to either function pies_on_map() or population_map(). You also need to be careful with the pies_on_map() function because the way it works is that the background tile is plotted and then I plot the pies ontop of it. If you reshape your plot window outside equal x- and y- coordinates (e.g., make it a non-square figure), the spatial location of the pie charts will move! This is a very frustrating thing but it has to do with the way viewports are overlain onto graphical objects in R and I have absolutely no control over it. So, the best option is to make the plot square, export it as PNG or TIFF or whaterver, then crop as necessary.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mapping Populations</span>"
    ]
  },
  {
    "objectID": "mapping_populations.html#dynamic-maps",
    "href": "mapping_populations.html#dynamic-maps",
    "title": "\n16  Mapping Populations\n",
    "section": "\n16.4 Dynamic maps",
    "text": "16.4 Dynamic maps\nWe can also produce dynamic maps for viewing in html output (like ebooks or webpages). Here I produce a leaflet map and then add markers on to the file using coordinates and put the clicked popup as the population name. This is an interactive map, you can zoom in, etc.\n\nlibrary(leaflet)\ncoords |&gt;\n  leaflet() |&gt;\n  addTiles() |&gt;\n  addMarkers( ~Longitude, ~Latitude, popup = ~Stratum)\n\n\n\n\n\nYou can change the underlying base map by passing on other Tiles to the construction. A list of available tile providers can be found here.\n\ncoords |&gt;\n  leaflet() |&gt;\n  addProviderTiles(\"Esri.WorldTopoMap\") |&gt;\n  addMarkers( ~Longitude, ~Latitude, popup=~Stratum )\n\n\n\n\n\nOr you can add custom images for the map relative to some meaningful information.\n\ncoords$Size &lt;- log(coords$Ae) * 10 + 1\ncoords |&gt;\n  leaflet() |&gt;\n  addProviderTiles(\"CartoDB.DarkMatter\") |&gt;\n  addCircleMarkers( ~Longitude, ~Latitude, popup=~Stratum, radius=~Size, color=\"red\", fillOpacity=0.75) \n\n\n\n\n\nIn the information that pops out of the marker, you can put a bit more information that may be helpful. Here I estimate some diversity measures (HO, HE, and AE).\n\nx &lt;- genetic_diversity(arapat,stratum = \"Population\",mode=\"Ho\")\nx &lt;- x[ x$Locus == \"LTRS\",]\ncoords &lt;- merge( coords, x[,c(1,3)])\nx &lt;- genetic_diversity(arapat,stratum = \"Population\",mode=\"He\")\nx &lt;- x[ x$Locus == \"LTRS\",]\ncoords &lt;- merge( coords, x[,c(1,3)])\nx &lt;- genetic_diversity(arapat,stratum = \"Population\",mode=\"Ae\")\nx &lt;- x[ x$Locus == \"LTRS\",]\ncoords &lt;- merge( coords, x[,c(1,3)])\n\nMake these data into a table format in html (OK it is a bit ugly but it is reasonable.)\n\ncoords$label &lt;- paste(\"&lt;h2&gt;\",coords$Stratum,\"&lt;/h2&gt;&lt;table&gt;&lt;tr&gt;&lt;th&gt;Parameter&lt;/th&gt;&lt;th&gt;Value&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;H&lt;sub&gt;O&lt;/sub&gt;&lt;/td&gt;&lt;td&gt;\", format(coords$Ho,digits = 3),\"&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;H&lt;sub&gt;E&lt;/sub&gt;&lt;/td&gt;&lt;td&gt;\",format(coords$He, digits=3), \"&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;A&lt;sub&gt;E&lt;/sub&gt;&lt;/td&gt;&lt;td&gt;\",format(coords$Ae,digits=3),\"&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;\",sep=\"\")\n\n# plot it\ncoords |&gt;\n  leaflet() |&gt;\n  addTiles() |&gt;\n  addCircleMarkers( ~Longitude, ~Latitude, popup=~label, radius=~Size, color=\"red\", fillOpacity=0.75) \n\n\n\n\n\n\n16.4.1 Sankey Networks\nA Sankey Diagram is a kind of flow diagram used to indicate allocation of one set of object into two other sets. This is a complicated set of interactions to display. In the following example, I’ll use the populations from the A. attenuata data set and then map each individual onto its STRUCTURE cluster and mtDNA clade. This plot needs a data.frame for nodes and one for edges created in a particular way.\n\nlibrary(networkD3)\npops &lt;- as.character(unique(arapat$Population))\nspecies &lt;- as.character(unique(arapat$Species))\ncluster &lt;- as.character(unique(arapat$Cluster))\n\nsankey.nodes &lt;- data.frame( name = c( pops, species, cluster), \n                            stringsAsFactors = FALSE)\n\n\n# takes vector of values and finds out the number of times\n# each combination is found in the pair.\nget_sets &lt;- function( source, target ) {\n  t &lt;- table(source, target)\n  m &lt;- which( t &gt; 0, arr.ind = TRUE)\n  suppressWarnings( df &lt;- data.frame( ind=m ) )\n  df$source &lt;- rownames(t)[df$ind.source]\n  df$target &lt;- colnames(t)[df$ind.target]\n  df$value &lt;- sapply(seq(1,dim(df)[1]), \n                     function(x) {\n                         return(t[df$ind.source[x],df$ind.target[x]])\n                         })\n  return( df[,3:5] )\n}\n\n# For finding the number of the name instead of the name\nnameIndex &lt;- function(vals,names){\n  ret &lt;- sapply( vals, function(x) {which(x==names)-1}, USE.NAMES = FALSE)\n  return(ret)\n}\n\n\n\nsankey.edges &lt;- rbind( get_sets(arapat$Species,arapat$Population),\n                       get_sets(arapat$Population, arapat$Cluster) )\nsankey.edges$targetNum &lt;- nameIndex(sankey.edges$target, sankey.nodes$name)\nsankey.edges$sourceNum &lt;- nameIndex(sankey.edges$source, sankey.nodes$name)\n\nWhat it does produce is a very cool and dynamic plot. The colored boxes represeent the groups–Species & STRUCTURE Cluster on the outside, population on the inside. Connecting lines indicate individuals who are in each of these partially overlapping groups. For example, if you grab the population Mat, you will see it has samples in it who are identified as either mtDNA type Peninsual or Cape as well as belonging to STRUCTURE groups CBP-C and SCBP-A. If you have difficult data such as these, this kind of plot may be very helpful.\n\nsankeyNetwork( Links=sankey.edges, Nodes=sankey.nodes,\n               Source = \"sourceNum\", Target = \"targetNum\",\n               Value = \"value\", NodeID = \"name\",\n               width = 700, fontSize = 12, nodeWidth = 30)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mapping Populations</span>"
    ]
  },
  {
    "objectID": "rarefaction.html",
    "href": "rarefaction.html",
    "title": "\n19  Rarefaction\n",
    "section": "",
    "text": "19.1 Mapping Diversity\nThe primary reason for looking at diversity is to perform some comparison, which provides some insights into the biological and/or demographic processes influencing your data. Without a basis for comparison, diversity estimates are just numbers. However, deriving an estimate of diversity is a statistical sampling process and as such we must be aware of the consequences our sampling regime has on the interpretation of the data. This is where rarefaction comes in, a technique commonly used in ecology when comparing species richness among groups.\nHere is an example of the problem sampling may interject into your analyses. Consider a single locus with four alleles.\nSelected as a random sample from an infinite population. The first sample has 5 individuals.\nAnd the second one has 100 individuals.\nThe difference in estimated diversity among these groups are abs( ae1 - ae2 ) = 0.15. Is this statistically different or are they the same? Is it just because we sampled more individuals in the second set that we get higher values of \\(A_e\\)? Consider the MP20 locus in the beetle data set, it has a total of 19 alleles present. If we subsample this data and estimate the number of observed alleles, we see that there is an asymptotic relationship between sampling effort and estimates of allelic diversity. Here is the code for estimating frequency independent diversity, \\(A\\), using these data.\nThe ‘curvy’ nature of this relationship shows a few things.\nIt takes a moderate sample size to capture the main set of alleles in the data set. If we are looking at allocating sampling using only 5 individuals per locale, then we are not going to get the majority of the alleles present.\nFor the rare alleles, you really need to grab large at-site samples if estimates of diversity are the main component of what you are doing. Do rare alleles aid in uncovering the biological processes you are interested in studying? They may or may not.\nFor most purposes, we will use all the samples we have collected. In many cases though, some locales may not have as many samples as other ones. So, even with these data, if I have one locale with 10 samples and another with 50, how can I determine if the differences observed are due to true differences in the underlying diversity and which are from my sampling? Just as in testing for HWE, we can use our new friend permutation to address the differences.\nRarefaction is the process of subsampling a larger dataset in smaller chunks such that we can estimate diversity among groups using the same number of individuals. Here is an example in the beetle dataset where I am going to look at differences in diversity among samples (\\(N = 75\\)) collected in the cape regions of Baja California\nand compare those to the genetic diversity observed from a smaller collection of individuals sampled from mainland Mexico (\\(N=36\\)).\nThe observed difference in effective allelic diversity, \\(A_{e,mainland} == A_{e,cape}\\), could be because the Cape region of Baja California is more diverse or it could be because there are twice as many individuals in that sample.\nTo perform a rarefaction on these data, we do the following:\nUse the size of the smallest population (\\(N\\)) as the sample size for all estimates. Randomly sample individuals, without replacement, from the larger dataset in allocations of size \\(N\\).\nEstimate diversity parameters on these subsamples and repeat to create a ‘null distribution’ of estimated diversity values.\nCompare your observed value in the smallest population to that distribution created by subsampling the larger population.\nFrom the data set, this is done by\nSo even if the samples sizes are the same, the mean level of diversity remains relatively constant. The range in diversity\nis quite large. Since this estimate is frequency based, random samples of alleles change the underlying estimate of Ae during each permutation.\nThe observed estimate of diversity in the Mainland populations does fall within this range. However, the null hypothesis states that \\(A_{e,mainland} = A_{e,cape}\\) and if this is true, once we standardize sample size, we can take the distribution of permuted Ae values as a statement about what we should see if the null hypothesis were true. As such, we can treat it probabilistically and estimate the probability that \\(A_e\\), Mainland is drawn from this distribution.\nOr graphically, it can be depicted as below.\nEstimating diversity is great and being able to compare two or more groups for their levels of diversity is even better. But often we are looking for spatial patterns in our data. Both R and gstudio provide easy interfaces for plotting data and later in the text we will see how to integrate raster and vector data into our workflows for more sublime approaches to characterizing population genetic processes. In the mean time, it is amazingly easy to use basic plotting commands to get pretty informative output. In this example, I extract the mean coordinate of each stratum in the arapat dataset and then estimate diversity at the level of these partitions and merge the diversity estimates for the AML locus into the coordinate data.frame.\nlibrary(gstudio)\ndata(arapat)\ndiversity &lt;- genetic_diversity(arapat, stratum=\"Population\", mode=\"Ae\")\ncoords &lt;- strata_coordinates(arapat)\ncoords &lt;- merge( coords, diversity[ diversity$Locus == \"AML\", ] )\nThen, I grab a map from the Google server and map my populations with diversity depicted as differences in the size of the points. Note that the ggmap() function provides the base map that is retrieved but when we use geom_point() we need to specify the aes() and the data= part as these data are from the data.frame we made, not from the map we grabbed from Google.\n#library(ggmap)\n#map &lt;- population_map(coords)\n#ggmap(map) + geom_point( aes(x=Longitude,y=Latitude,size=Ae), data=coords)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Rarefaction</span>"
    ]
  },
  {
    "objectID": "distance_matrices.html",
    "href": "distance_matrices.html",
    "title": "\n25  Distance Matrices\n",
    "section": "",
    "text": "25.1 Visual Comparisons\nIn this section, we examined several different ways in which we can estimate pairwise genetic distances. In this section, we examine ways in which we can gain some statistical confidence in the way that we can compare the components of genetic matrices.\nIn the discussion below, we will be comparing estimates of population-level genetic distance as determined using both Nei’s genetic distance and Euclidean based upon allele frequencies.\nAt a first pass, the most parsimonious approach to understanding the relationship between elements of two matrices would be to plot them. To visualize, we only need the lower (or upper) triangle of each matrix since they are symmetric.\nlibrary(ggplot2)\ndf &lt;- data.frame( Nei=D1[lower.tri(D1)], Euclidean=D2[ lower.tri(D2)])\nsummary(df)\n\n      Nei             Euclidean     \n Min.   :0.009049   Min.   :0.3182  \n 1st Qu.:0.250710   1st Qu.:1.5684  \n Median :0.633049   Median :2.2668  \n Mean   :0.645306   Mean   :2.0703  \n 3rd Qu.:0.937829   3rd Qu.:2.6145  \n Max.   :2.427508   Max.   :3.4936  \n\nggplot(df,aes(x=Nei,y=Euclidean)) + geom_point()\nFrom this, we can see that both methods produce a set of distances that are largely similar, though not linear. Remember that Nei’s distance is linear in time only in relation to mutation drift equilibrium.\nSo lets expand on this output a bit. As we’ve seen in this dataset, there are at least three different groups in the data that have been designated as different ‘species’ using mtDNA (see Garrick et al. 2013 for more on this). It would be helpful if the way in which we plot these population comparisons would indicate if the comparisons were among the same species groups or among different species groups. Are the larger estimates of genetic distance more likely between these species groups?\nTo do this, we need to collapse the values in arapat$Species by population.\nassignments &lt;- by( arapat$Species, arapat$Population, unique)\nThis gives a list of values for each population of the unique entries from arapat$Species. Some populations are completely one type\nassignments[1]\n\n$`101`\n[1] Mainland\nLevels: Cape Mainland Peninsula\nwhereas others are mixed between two types of species\nassignments[6]\n\n$`157`\n[1] Cape      Peninsula\nLevels: Cape Mainland Peninsula\nTo capture the different combinations of groupings, we can define a small function that takes the species designations and concatenates them together—instead of having a 2-element vector of “Cape” and “Peninsula” for population 157, we have a “Cape/Peninsula” designation. To prevent “Cape/Peninsula” and “Peninsula/Cape” from being different just due to the order in which they are found in the data.frame, I sort them first.\nfoo &lt;- function( x ) { return( paste( sort( unique(x) ) , collapse=\"/\") ) }\nassignments &lt;- by( arapat$Species, arapat$Population, foo)\nassignments[1]\n\n       101 \n\"Mainland\" \n\nassignments[6]\n\n             157 \n\"Cape/Peninsula\"\nThis gives us a vector of species composition equal in length to the number of populations in the data set. However, the comparisons we are making are not based on the number of populations, they are based upon the pair-wise comparison of populations. In these data, we have 39 populations but the scatter plot of pairwise comparisons in genetic distance has \\(\\frac{39(39-1)}{2} = 741\\) points on it. As a result, we need to take this vector of assignments and create a pairwise comparison. We can do this using the outer() function, which allows us to take two vectors of equal length and perform some operation on them to produce a square matrix. This is exactly how vector multiplication works, only instead of multiplying vectors of numbers, we can tell it to paste together the assignments.\nassignments &lt;- as.character( assignments)\nC &lt;- outer( assignments, assignments, FUN=paste)\nC[1:3,1:3]\n\n     [,1]                 [,2]                 [,3]                 \n[1,] \"Mainland Mainland\"  \"Mainland Mainland\"  \"Mainland Peninsula\" \n[2,] \"Mainland Mainland\"  \"Mainland Mainland\"  \"Mainland Peninsula\" \n[3,] \"Peninsula Mainland\" \"Peninsula Mainland\" \"Peninsula Peninsula\"\nNow we can plot it as normal.\ndf$Species &lt;- C[ lower.tri(C) ]\nggplot(df,aes(x=Nei,y=Euclidean, color=Species)) + geom_point()\nwhich produces a somewhat confusing melange of output. Perhaps all potential combinations provide a little too much information to see overall trends. Perhaps it is sufficient to partition the points into comparisons among populations that are of the same composition from those that are comparing different species compositions.\ndf$Status &lt;- df$Species\ndf$Status[ df$Species == \"Cape Cape\"] &lt;- \"Same\"\ndf$Status[ df$Species == \"Mainland Mainland\"] &lt;- \"Same\"\ndf$Status[ df$Species == \"Peninsula Peninsula\"] &lt;- \"Same\"\ndf$Status[ df$Status != \"Same\" ] &lt;- \"Mixed\"\nggplot(df,aes(x=Nei,y=Euclidean, color=Status)) + geom_point() + facet_grid(Status~.)\n\n\n\n\n\n\nFigure 25.1: Pairwise genetic distance among populations of Arapat attenuatus partitioned by species composition within each population.\nGiven the density of the plots and the relative numbers of them (there are more Mixed than Same) plotting the data like this may not reveal all the underlying trends. Another way to augment this graphical output would be to add marginal density distributions to the plot. To do this, we can create a grid of plots using the ggExtra grid.arrange() function. To do this, we need to create a few different plots and then put them together into a single output. A single scatter plot will have both types of points in it.\nscatter &lt;- ggplot(df,aes(x=Nei,y=Euclidean, color=Status)) + geom_point() +\n  theme(legend.position=c(1,1),legend.justification=c(1,1))\nThen we need to create the density for Nei’s distance to be plot on top of the scatter plot\nplot_top &lt;- ggplot( df, aes(x=Nei,fill=Status)) + geom_density(alpha=0.5) + theme(legend.position=\"none\")\nand the corresponding one for the Euclidean distance (notice I’m flipping the coordinates here because I want the density to be along the right side of the graph to mirror the y-axis.\nplot_side &lt;- ggplot( df, aes(x=Euclidean, fill=Status)) + geom_density(alpha=0.5) + coord_flip() + theme(legend.position=\"none\")\nI also need an empty plot to since we are going to make a 2x2 grid of plots but the upper right one will have nothing in it (the popgraph library has a nice invisible theme called theme_empty()).\nlibrary(popgraph)\nempty &lt;- ggplot() + theme_minimal()\nFinally, we can stitch this all together and produce a single plot with these four subcomponents.\nlibrary(gridExtra)\ngrid.arrange(plot_top, empty, scatter, plot_side, ncol=2, nrow=2, widths=c(4,1), heights=c(1,4))\n\n\n\n\n\n\nFigure 25.2: Pairwise genetic distance measured using Nei’s distance and Euclidean distance based upon allele frequencies for populations of Araptus attenuatus. Each comparison is classified as being between the same species group or between mixed groups and marginal probability densities are shown for each measure.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Distance Matrices</span>"
    ]
  },
  {
    "objectID": "distance_matrices.html#the-mantel-test",
    "href": "distance_matrices.html#the-mantel-test",
    "title": "\n25  Distance Matrices\n",
    "section": "\n25.2 The Mantel Test",
    "text": "25.2 The Mantel Test\nBy far, the most common approach for estimating the relative similarity in two matrices is through the use of a Mantel Test. This test was originally developed, by Nathan Mantel, to estimate the correlation between two matrices, \\(\\bfseries{X}\\) and \\(\\bfseries{Y}\\). The idea here is that if there is a positive association between the elements in each matrix, then the large values in \\(\\bfseries{X}\\) will be matched up with large values in \\(\\bfseries{Y}\\) and the small values in \\(\\bfseries{X}\\) will be matched up with the small values in \\(\\bfseries{Y}\\). Conversely, if there is a negative relationship between the two, the large values in \\(\\bfseries{X}\\) are matched with the small values in \\(\\bfseries{Y}\\) and vice-versa. As such, we estimate the sum of the multiplication between each of the corresponding elements in these matrices (assuming they are both square and have \\(N\\) rows and columns) as:\n[ Z = {i=1}^N{j=1}^N x_{ij}y_{ij} ]\nThe expected value of \\(Z\\) is unknown since the forces that shape the relationship between these two matrices is largely unknown. As a consequence, we cannot look up a value of \\(Z\\) to determine its probability. However, we can use our old friend permutation to set up some simulations that may help out understanding if the estimated value of \\(Z\\) is small or large, given potential values it could take on with these two matrices. So what we do is take one of the matrices and shuffle the rows and columns and estimate permuted values of [~] and then compare our observed estimate to these permuted values. Again, like we did in the section on rarefaction, we take the null hypothesis, \\(H_O:\\) There is no relationship between elements of these two matrices, and assume it is true. If it is in fact true, then any permutation of one of the matrices should produce values of \\(\\tilde{Z}\\) as large in magnitude as any other permutation. It is only where we have significant correlations, either positive or negative, between the matrices that we have values of \\(\\tilde{Z}\\) very large or very small. Therefore, we should be able to define a level of confidence in the magnitude of our observed \\(Z\\) by defining a large set of permuted \\(\\tilde{Z}\\) values and comparing them. This is exactly how it is done1.\nIn a paper by Smouse et al. (1986), they showed that this approach is basically just another way of doing a partial regression analysis. The importance of this observation is that while a particular value of \\(Z\\) may be examined for significance, it is a scaleless entity and varies with the magnitude of the values in the individual matrices. As such, it is difficult to interpret. Is \\(Z=116\\) a large value? However, if interpreted in a regression context, we can estimate scaled parameters such as the partial correlation coefficients. The math is pretty easy here. The sums of squares for both \\(\\bfseries{X}\\) and \\(\\bfseries{X}\\) are found as:\n[ SS_X = {i=1}^N{j=1}^N(x_{ij} - {x})^2 ]\nand\n[ SS_X = {i=1}^N{j=1}^N(x_{ij} - {x})^2 ]\nwhere \\(\\bar{x} = \\frac{\\sum_{i=1}^N\\sum_{j=1}^N x_{ij}}{N(N-1)}\\) and \\(\\bar{y} = \\frac{\\sum_{i=1}^N\\sum_{j=1}^N y_{ij}}{N(N-1)}\\), and the sums of cross products are defined as:\n[ SS_{XY} = Z - N{x}{y} ]\nThis means that the correlation coefficient is\n[ = ]\nwhich is essentially the Pearson product moment correlation coefficient! How convenient. They go on to extend this work to show how if there are more than one ‘predictor’ matrix here, we can also derive the partial correlation coefficients, \\(\\rho_{x_1|x_2}\\) (e.g., the coorelation of \\(x_1\\) after fixing the effects of \\(x_2\\)) and other useful components. In R, we have this kind of work done for us rather easily. There are several packages that provide a Mantel test, we will use the one out of the vegan package.\nIn this example, the distance matrices must first be translated into dist objects (a type of symmetrical matrix).\n\n library(vegan)\nmantel( as.dist(D1), as.dist(D2), permutations=9999 )\n\n\nMantel statistic based on Pearson's product-moment correlation \n\nCall:\nmantel(xdis = as.dist(D1), ydis = as.dist(D2), permutations = 9999) \n\nMantel statistic r: 0.9373 \n      Significance: 1e-04 \n\nUpper quantiles of permutations (null model):\n   90%    95%  97.5%    99% \n0.0718 0.0957 0.1166 0.1423 \nPermutation: free\nNumber of permutations: 9999\n\n\nFrom this output, we see that the two distance matrices are highly correlated. The question that I’ll leave the reader to look at is one based upon the shape of all those pairwise graphs. Is this a linear relationship? Is it ok to use a Pearson correlation on these data?\nRecent work by Legendre & Fortin (2010) has suggested that in some situations the use of a Mantel test results in bias assocaited with ascertaining probabilities.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Distance Matrices</span>"
    ]
  },
  {
    "objectID": "distance_matrices.html#footnotes",
    "href": "distance_matrices.html#footnotes",
    "title": "\n25  Distance Matrices\n",
    "section": "",
    "text": "And coincidently, how much of the statistical testing is done in population genetics since we have no idea what the underlying distributions are expected to be.↩︎",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Distance Matrices</span>"
    ]
  },
  {
    "objectID": "distance_networks.html",
    "href": "distance_networks.html",
    "title": "\n27  Distance Networks\n",
    "section": "",
    "text": "27.1 Graph Structure\nA &lt;- matrix(0, nrow=5, ncol=5)\nA[1,2] &lt;- A[2,3] &lt;- A[1,3] &lt;- A[3,4] &lt;- A[4,5] &lt;- 1\nA &lt;- A + t(A)\nA\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    1    1    0    0\n[2,]    1    0    1    0    0\n[3,]    1    1    0    1    0\n[4,]    0    0    1    0    1\n[5,]    0    0    0    1    0\nThere is a quick function, as.popgraph() that takes either an existing igraphobject or a matrix and turns them into popgraph objects.\nlibrary(igraph)\ng &lt;- graph_from_adjacency_matrix( A , mode=\"undirected\")\nThere are several options available under the mode parameter. We typically use the undirected graph option but the following are also available:\nThe graph object presents several characteristics as an output including the number of nodes and edges, the mode of the graph, and a list of the edges (if there aren’t too many–in which case the list is truncated).\ng\n\nIGRAPH 25546d3 U--- 5 5 -- \n+ edges from 25546d3:\n[1] 1--2 1--3 2--3 3--4 4--5\nHere we see that this object is an igraph, is ’U’ndirected, and has 5 nodes and 5 edges. The edges are indicated by numbers and are graphically displayed.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Distance Networks</span>"
    ]
  },
  {
    "objectID": "distance_networks.html#graph-structure",
    "href": "distance_networks.html#graph-structure",
    "title": "\n27  Distance Networks\n",
    "section": "",
    "text": "undirected The connections between nodes are symmetric. This is the default for population graphs as covariance, the quantity the edge is representing is symmetrical.\n\ndirected The edges are asymetric.\n\nmax or min Will take the largest (or smallest) value of the matrix (e.g., \\(max(A[i,j], A[j,i])\\) or \\(min( A[i,j], A[j,i])\\) ).\n\nupper or lower Uses either the upper or lower element of the matrix.\n\nplus Adds upper and lower values (e.g., \\(A[i,j] + A[j,i]\\)).\n\n\n\n\n\n27.1.1 Node & Edge Attributes\nThe underlying structure of an igraph object allows you to assoicate attributes (e.g., other data) with nodes and edges. Node attributes are accessed using the \\(V(graph)\\) operator (for vertex) and edge attributes are done via \\(E(graph)\\). Attributes can be set as well as retrieved using the same mechanisms for nodes.\n\nV(g)$name &lt;- c(\"Olympia\",\"Bellingham\",\"St. Louis\",\"Ames\",\"Richmond\")\nV(g)$group &lt;- c(\"West\",\"West\", \"Central\",\"Central\",\"East\")\nV(g)$color &lt;- \"#cca160\"\nlist.vertex.attributes( g )\n\n[1] \"name\"  \"group\" \"color\"\n\nV(g)$name\n\n[1] \"Olympia\"    \"Bellingham\" \"St. Louis\"  \"Ames\"       \"Richmond\"  \n\n\nand for edges\n\nE(g)\n\n+ 5/5 edges from 25546d3 (vertex names):\n[1] Olympia   --Bellingham Olympia   --St. Louis  Bellingham--St. Louis \n[4] St. Louis --Ames       Ames      --Richmond  \n\nE(g)$color &lt;- c(\"red\",\"red\", \"red\", \"blue\",\"dark green\")\nlist.edge.attributes( g )\n\n[1] \"color\"",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Distance Networks</span>"
    ]
  },
  {
    "objectID": "distance_networks.html#plotting-a-graphs",
    "href": "distance_networks.html#plotting-a-graphs",
    "title": "\n27  Distance Networks\n",
    "section": "\n27.2 Plotting a Graphs",
    "text": "27.2 Plotting a Graphs\nOne of the main benefits to using R is that you can leverage the mutlitude of other packages to visualize and manipulate your data in interesting and informative ways. Since a popgraph is an instance of an igraph element, we can use the igraph routines for plotting. Here is an example.\n\nplot(g)\n\n\n\n\n\n\n\nThere are several different options you can use to manipulate the graphical forms. By default, the plotting routines look for node and edge attributes such as name and color to plot the output appropriately. There are several additional plotting functions for plotting igraph objects. Here are some examples.\n\nplot(g, edge.color=\"black\", vertex.label.color=\"darkred\", vertex.color=\"#cccccc\", vertex.label.dist=1)\n\n\n\n\n\n\n\nBelow is the output from the igraph.plotting help file that covers the main options that you have for customizing the way that the network is displayed.\n\nDrawing graphs {igraph} R Documentation\nDrawing graphs\n\nDescription\n\nThe common bits of the three plotting functions plot.igraph, tkplot and rglplot are discussed in this manual page\n\nDetails\n\nThere are currently three different functions in the igraph package which can draw graph in various ways:\n\nplot.igraph does simple non-interactive 2D plotting to R devices. Actually it is an implementation of the plot generic function, so you can write plot(graph) instead of plot.igraph(graph). As it used the standard R devices it supports every output format for which R has an output device. The list is quite impressing: PostScript, PDF files, XFig files, SVG files, JPG, PNG and of course you can plot to the screen as well using the default devices, or the good-looking anti-aliased Cairo device. See plot.igraph for some more information.\n\ntkplot does interactive 2D plotting using the tcltk package. It can only handle graphs of moderate size, a thousend vertices is probably already too many. Some parameters of the plotted graph can be changed interactively after issuing the tkplot command: the position, color and size of the vertices and the color and width of the edges. See tkplot for details.\n\nrglplot is an experimental function to draw graphs in 3D using OpenGL. See rglplot for some more information.\n\nPlease also check the examples below.\n\nHow to specify graphical parameters\n\nThere are three ways to give values to the parameters described below, in section 'Parameters'. We give these three ways here in the order of their precedence.\n\nThe first method is to supply named arguments to the plotting commands: plot.igraph, tkplot or rglplot. Parameters for vertices start with prefix 'vertex.', parameters for edges have prefix 'edge.', and global parameters have no prefix. Eg. the color of the vertices can be given via argument vertex.color, whereas edge.color sets the color of the edges. layout gives the layout of the graphs.\n\nThe second way is to assign vertex, edge and graph attributes to the graph. These attributes have no prefix, ie. the color of the vertices is taken from the color vertex attribute and the color of the edges from the color edge attribute. The layout of the graph is given by the layout graph attribute. (Always assuming that the corresponding command argument is not present.) Setting vertex and edge attributes are handy if you want to assign a given 'look' to a graph, attributes are saved with the graph is you save it with save or in GraphML format with write_graph, so the graph will have the same look after loading it again.\n\nIf a parameter is not given in the command line, and the corresponding vertex/edge/graph attribute is also missing then the general igraph parameters handled by igraph_options are also checked. Vertex parameters have prefix 'vertex.', edge parameters are prefixed with 'edge.', general parameters like layout are prefixed with 'plot'. These parameters are useful if you want all or most of your graphs to have the same look, vertex size, vertex color, etc. Then you don't need to set these at every plotting, and you also don't need to assign vertex/edge attributes to every graph.\n\nIf the value of a parameter is not specified by any of the three ways described here, its default valued is used, as given in the source code.\n\nDifferent parameters can have different type, eg. vertex colors can be given as a character vector with color names, or as an integer vector with the color numbers from the current palette. Different types are valid for different parameters, this is discussed in detail in the next section. It is however always true that the parameter can always be a function object in which it will be called with the graph as its single argument to get the \"proper\" value of the parameter. (If the function returns another function object that will not be called again...)\n\nThe list of parameters\n\nVertex parameters first, note that the 'vertex.' prefix needs to be added if they are used as an argument or when setting via igraph_options. The value of the parameter may be scalar valid for every vertex or a vector with a separate value for each vertex. (Shorter vectors are recycled.)\n\nsize\nThe size of the vertex, a numeric scalar or vector, in the latter case each vertex sizes may differ. This vertex sizes are scaled in order have about the same size of vertices for a given value for all three plotting commands. It does not need to be an integer number.\n\nThe default value is 15. This is big enough to place short labels on vertices.\n\nsize2\nThe \"other\" size of the vertex, for some vertex shapes. For the various rectangle shapes this gives the height of the vertices, whereas size gives the width. It is ignored by shapes for which the size can be specified with a single number.\n\nThe default is 15.\n\ncolor\nThe fill color of the vertex. If it is numeric then the current palette is used, see palette. If it is a character vector then it may either contain integer values, named colors or RGB specified colors with three or four bytes. All strings starting with '#' are assumed to be RGB color specifications. It is possible to mix named color and RGB colors. Note that tkplot ignores the fourth byte (alpha channel) in the RGB color specification.\n\nFor plot.igraph and integer values, the default igraph palette is used (see the 'palette' parameter below. Note that this is different from the R palette.\n\nIf you don't want (some) vertices to have any color, supply NA as the color name.\n\nThe default value is \"SkyBlue2\".\n\nframe.color\nThe color of the frame of the vertices, the same formats are allowed as for the fill color.\n\nIf you don't want vertices to have a frame, supply NA as the color name.\n\nBy default it is \"black\".\n\nshape\nThe shape of the vertex, currently \"circle\", \"square\", \"csquare\", \"rectangle\", \"crectangle\", \"vrectangle\", \"pie\" (see vertex.shape.pie), 'sphere', and \"none\" are supported, and only by the plot.igraph command. \"none\" does not draw the vertices at all, although vertex label are plotted (if given). See shapes for details about vertex shapes and vertex.shape.pie for using pie charts as vertices.\n\nThe \"sphere\" vertex shape plots vertices as 3D ray-traced spheres, in the given color and size. This produces a raster image and it is only supported with some graphics devices. On some devices raster transparency is not supported and the spheres do not have a transparent background. See dev.capabilities and the 'rasterImage' capability to check that your device is supported.\n\nBy default vertices are drawn as circles.\n\nlabel\nThe vertex labels. They will be converted to character. Specify NA to omit vertex labels.\n\nThe default vertex labels are the vertex ids.\n\nlabel.family\nThe font family to be used for vertex labels. As different plotting commands can used different fonts, they interpret this parameter different ways. The basic notation is, however, understood by both plot.igraph and tkplot. rglplot does not support fonts at all right now, it ignores this parameter completely.\n\nFor plot.igraph this parameter is simply passed to text as argument family.\n\nFor tkplot some conversion is performed. If this parameter is the name of an exixting Tk font, then that font is used and the label.font and label.cex parameters are ignored complerely. If it is one of the base families (serif, sans, mono) then Times, Helvetica or Courier fonts are used, there are guaranteed to exist on all systems. For the 'symbol' base family we used the symbol font is available, otherwise the first font which has 'symbol' in its name. If the parameter is not a name of the base families and it is also not a named Tk font then we pass it to tkfont.create and hope the user knows what she is doing. The label.font and label.cex parameters are also passed to tkfont.create in this case.\n\nThe default value is 'serif'.\n\nlabel.font\nThe font within the font family to use for the vertex labels. It is interpreted the same way as the the font graphical parameter: 1 is plain text, 2 is bold face, 3 is italic, 4 is bold and italic and 5 specifies the symbol font.\n\nFor plot.igraph this parameter is simply passed to text.\n\nFor tkplot, if the label.family parameter is not the name of a Tk font then this parameter is used to set whether the newly created font should be italic and/or boldface. Otherwise it is ignored.\n\nFor rglplot it is ignored.\n\nThe default value is 1.\n\nlabel.cex\nThe font size for vertex labels. It is interpreted as a multiplication factor of some device-dependent base font size.\n\nFor plot.igraph it is simply passed to text as argument cex.\n\nFor tkplot it is multiplied by 12 and then used as the size argument for tkfont.create. The base font is thus 12 for tkplot.\n\nFor rglplot it is ignored.\n\nThe default value is 1.\n\nlabel.dist\nThe distance of the label from the center of the vertex. If it is 0 then the label is centered on the vertex. If it is 1 then the label is displayed beside the vertex.\n\nThe default value is 0.\n\nlabel.degree\nIt defines the position of the vertex labels, relative to the center of the vertices. It is interpreted as an angle in radian, zero means 'to the right', and 'pi' means to the left, up is -pi/2 and down is pi/2.\n\nThe default value is -pi/4.\n\nlabel.color\nThe color of the labels, see the color vertex parameter discussed earlier for the possible values.\n\nThe default value is black.\n\nEdge parameters require to add the 'edge.' prefix when used as arguments or set by igraph_options. The edge parameters:\n\ncolor\nThe color of the edges, see the color vertex parameter for the possible values.\n\nBy default this parameter is darkgrey.\n\nwidth\nThe width of the edges.\n\nThe default value is 1.\n\narrow.size\nThe size of the arrows. Currently this is a constant, so it is the same for every edge. If a vector is submitted then only the first element is used, ie. if this is taken from an edge attribute then only the attribute of the first edge is used for all arrows. This will likely change in the future.\n\nThe default value is 1.\n\narrow.width\nThe width of the arrows. Currently this is a constant, so it is the same for every edge. If a vector is submitted then only the first element is used, ie. if this is taken from an edge attribute then only the attribute of the first edge is used for all arrows. This will likely change in the future.\n\nThis argument is currently only used by plot.igraph.\n\nThe default value is 1, which gives the same width as before this option appeared in igraph.\n\nlty\nThe line type for the edges. Almost the same format is accepted as for the standard graphics par, 0 and \"blank\" mean no edges, 1 and \"solid\" are for solid lines, the other possible values are: 2 (\"dashed\"), 3 (\"dotted\"), 4 (\"dotdash\"), 5 (\"longdash\"), 6 (\"twodash\").\n\ntkplot also accepts standard Tk line type strings, it does not however support \"blank\" lines, instead of type '0' type '1', ie. solid lines will be drawn.\n\nThis argument is ignored for rglplot.\n\nThe default value is type 1, a solid line.\n\nlabel\nThe edge labels. They will be converted to character. Specify NA to omit edge labels.\n\nEdge labels are omitted by default.\n\nlabel.family\nFont family of the edge labels. See the vertex parameter with the same name for the details.\n\nlabel.font\nThe font for the edge labels. See the corresponding vertex parameter discussed earlier for details.\n\nlabel.cex\nThe font size for the edge labels, see the corresponding vertex parameter for details.\n\nlabel.color\nThe color of the edge labels, see the color vertex parameters on how to specify colors.\n\nlabel.x\nThe horizontal coordinates of the edge labels might be given here, explicitly. The NA elements will be replaced by automatically calculated coordinates. If NULL, then all edge horizontal coordinates are calculated automatically. This parameter is only supported by plot.igraph.\n\nlabel.y\nThe same as label.x, but for vertical coordinates.\n\ncurved\nSpecifies whether to draw curved edges, or not. This can be a logical or a numeric vector or scalar.\n\nFirst the vector is replicated to have the same length as the number of edges in the graph. Then it is interpreted for each edge separately. A numeric value specifies the curvature of the edge; zero curvature means straight edges, negative values means the edge bends clockwise, positive values the opposite. TRUE means curvature 0.5, FALSE means curvature zero.\n\nBy default the vector specifying the curvatire is calculated via a call to the curve_multiple function. This function makes sure that multiple edges are curved and are all visible. This parameter is ignored for loop edges.\n\nThe default value is FALSE.\n\nThis parameter is currently ignored by rglplot.\n\narrow.mode\nThis parameter can be used to specify for which edges should arrows be drawn. If this parameter is given by the user (in either of the three ways) then it specifies which edges will have forward, backward arrows, or both, or no arrows at all. As usual, this parameter can be a vector or a scalar value. It can be an integer or character type. If it is integer then 0 means no arrows, 1 means backward arrows, 2 is for forward arrows and 3 for both. If it is a character vector then \"\" and \"-&gt;\" forward arrows and \"\" and \"\" stands for both arrows. All other values mean no arrows, perhaps you should use \"-\" or \"–\" to specify no arrows.\n\nHint: this parameter can be used as a 'cheap' solution for drawing \"mixed\" graphs: graphs in which some edges are directed some are not. If you want do this, then please create a directed graph, because as of version 0.4 the vertex pairs in the edge lists can be swapped in undirected graphs.\n\nBy default, no arrows will be drawn for undirected graphs, and for directed graphs, an arrow will be drawn for each edge, according to its direction. This is not very surprising, it is the expected behavior.\n\nloop.angle\nGives the angle in radian for plotting loop edges. See the label.dist vertex parameter to see how this is interpreted.\n\nThe default value is 0.\n\nloop.angle2\nGives the second angle in radian for plotting loop edges. This is only used in 3D, loop.angle is enough in 2D.\n\nThe default value is 0.\n\nOther parameters:\n\nlayout\nEither a function or a numeric matrix. It specifies how the vertices will be placed on the plot.\n\nIf it is a numeric matrix, then the matrix has to have one line for each vertex, specifying its coordinates. The matrix should have at least two columns, for the x and y coordinates, and it can also have third column, this will be the z coordinate for 3D plots and it is ignored for 2D plots.\n\nIf a two column matrix is given for the 3D plotting function rglplot then the third column is assumed to be 1 for each vertex.\n\nIf layout is a function, this function will be called with the graph as the single parameter to determine the actual coordinates. The function should return a matrix with two or three columns. For the 2D plots the third column is ignored.\n\nThe default value is layout_nicely, a smart function that chooses a layouter based on the graph.\n\nmargin\nThe amount of empty space below, over, at the left and right of the plot, it is a numeric vector of length four. Usually values between 0 and 0.5 are meaningful, but negative values are also possible, that will make the plot zoom in to a part of the graph. If it is shorter than four then it is recycled.\n\nrglplot does not support this parameter, as it can zoom in and out the graph in a more flexible way.\n\nIts default value is 0.\n\npalette\nThe color palette to use for vertex color. The default is categorical_pal, which is a color-blind friendly categorical palette. See its manual page for details and other palettes. This parameters is only supported by plot, and not by tkplot and rglplot.\n\nrescale\nLogical constant, whether to rescale the coordinates to the [-1,1]x[-1,1](x[-1,1]) interval. This parameter is not implemented for tkplot.\n\nDefaults to TRUE, the layout will be rescaled.\n\nasp\nA numeric constant, it gives the asp parameter for plot, the aspect ratio. Supply 0 here if you don't want to give an aspect ratio. It is ignored by tkplot and rglplot.\n\nDefaults to 1.\n\nframe\nBoolean, whether to plot a frame around the graph. It is ignored by tkplot and rglplot.\n\nDefaults to FALSE.\n\nmain\nOverall title for the main plot. The default is empty if the annotate.plot igraph option is FALSE, and the graph's name attribute otherwise. See the same argument of the base plot function. Only supported by plot.\n\nsub\nSubtitle of the main plot, the default is empty. Only supported by plot.\n\nxlab\nTitle for the x axis, the default is empty if the annotate.plot igraph option is FALSE, and the number of vertices and edges, if it is TRUE. Only supported by plot.\n\nylab\nTitle for the y axis, the default is empty. Only supported by plot.\n\nAuthor(s)\n\nGabor Csardi csardi.gabor@gmail.com\n\nSee Also\n\nplot.igraph, tkplot, rglplot, igraph_options\n\nExamples\n\n## Not run:\n\n# plotting a simple ring graph, all default parameters, except the layout\ng \n\n\nIn addition to the physical appearance of nodes, edges, and labels, networks are must also have a ‘layout’ that describes the relative position of nodes on the plot surface. There are several ways you can define a layout, here are some examples.\n\nlayout &lt;- layout.circle( g )\nplot( g, layout=layout)\n\n\n\n\n\n\nlayout &lt;- layout.fruchterman.reingold( g )\nplot( g, layout=layout)\n\n\n\n\n\n\n\nIn addition to normal plotting, you can also integrate interactive plotting. Here is an example using the networkD3 library. It is interactive, so grab one of the nodes and move it around.\n\nlibrary(networkD3)\nedgelist &lt;- as_edgelist(g)\ndf &lt;- data.frame( src=edgelist[,1], target=edgelist[,2])\nsimpleNetwork(df,fontSize = 14,opacity = 0.95)",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Distance Networks</span>"
    ]
  },
  {
    "objectID": "distance_networks.html#mapping-networks",
    "href": "distance_networks.html#mapping-networks",
    "title": "\n27  Distance Networks\n",
    "section": "\n27.3 Mapping Networks",
    "text": "27.3 Mapping Networks\nFor quick maps I typically use the maps library. It is pretty straightforward to use and does not take too much thought to quickly plot something or find the approporiate raster files. Below, I add some coordinates to the data set.\n\nV(g)$Latitude &lt;- c( 47.15, 48.75,38.81, 42.26, 37.74 )\nV(g)$Longitude &lt;- c(-122.89,-122.49,-89.98, -93.47, -77.16 )\n\nThen overlay this onto a map using the overlay_popgraph() function. Here is an example where I plot it over the map of the US states.\n\nlibrary(maps)\nlibrary(popgraph)\npg &lt;- as.popgraph( g )\n\nmap( \"state\" )\n# overlay_popgraph( pg )\n\n\n\n\n\n\nFigure 27.1: Map of graph stetched onto continential US map.\n\n\n\n\nThis function requires that you already have a plot available (it uses the lines() and points() routines). If you try to just overlay this with no existing plot, it will not work (and should throw an error).",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Distance Networks</span>"
    ]
  },
  {
    "objectID": "distance_networks.html#genetic-distance-graphs",
    "href": "distance_networks.html#genetic-distance-graphs",
    "title": "\n27  Distance Networks\n",
    "section": "\n27.4 Genetic Distance Graphs",
    "text": "27.4 Genetic Distance Graphs\nSeveral graph-theoretic approaches have been suggested in the literature, some of which are based upon statistical models (e.g., popgraphs in the next chapter) and some of which are less structured.\nA common approach has been to use a measure of pair-wise genetic distance, measured between individuals or strata. In the following example, Nei’s genetic distance (see Chapter @ref(genetic-distance)) is used.\n\n library(gstudio)\ndata(arapat)\nd &lt;- genetic_distance(arapat,mode=\"Nei\")\n\nyielding a \\(KxK\\) distance matrix. Nei’s distance produces values that are non-negative\n\nd[1:6,1:6]\n\n          101      102        12       153        156        157\n101 0.0000000 0.226820 0.8385165 0.9177730 1.19671667 1.09526743\n102 0.2268200 0.000000 1.2589813 1.1874872 1.29512009 1.25632140\n12  0.8385165 1.258981 0.0000000 0.1340892 1.22979312 0.90185134\n153 0.9177730 1.187487 0.1340892 0.0000000 1.14458359 0.86975864\n156 1.1967167 1.295120 1.2297931 1.1445836 0.00000000 0.03459425\n157 1.0952674 1.256321 0.9018513 0.8697586 0.03459425 0.00000000\n\n\nand in the case of the arapat data, produces a bivariate distribution of distances.\n\n library(ggplot2)\nqplot(d[lower.tri(d)],geom = \"histogram\", bins=40)\n\n\n\n\n\n\n\nSince Nei’s distance is",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Distance Networks</span>"
    ]
  },
  {
    "objectID": "population_graphs.html",
    "href": "population_graphs.html",
    "title": "\n28  Population Graphs\n",
    "section": "",
    "text": "28.1 Adding data to a graph\nPopulation Graphs are a statistical representation of among population genetic variance, \\(\\sigma^2_A\\), as viewed through a network (Dyer & Nason 2004). A population graph is a graph-theoretic interpretation of genetic covariance and serves as a tool for understanding underlying evolutionary history for a set of populations.\nThese structures are estimated in R using the popgraphs library.\nAs other networks, a population graph is a graph-theoretic structure that can be represetendHere we will focus on the former approach as it is native to this package. If you use the latter one, it will produce a *.pgraph file and you can read it in using the read_popgraph() function.\nA population graph is made more informative if you can associate some data with topology. External data may be spatial or ecolgoical data associated with each node. Edge data may be a bit more complicated as it is traversing both spatial and ecolgoical gradients and below we’ll see how to extract particular from rasters using edge crossings.\nIncluded in the popgraph package are some build-in data sets to illustrate some of the utility. Included is the cactus topology that was originally used to develop this approach (from Dyer & Nason 2004).\ndata(lopho)\nclass(lopho)\n\n[1] \"popgraph\" \"igraph\"  \n\nlopho\n\nIGRAPH 6af7beb UNW- 21 52 -- \n+ attr: name (v/c), size (v/n), color (v/c), Region (v/c), weight (e/n)\n+ edges from 6af7beb (vertex names):\n [1] BaC--LaV    BaC--Lig    BaC--PtC    BaC--PtP    BaC--SnE    BaC--SnI   \n [7] BaC--StR    Ctv--PtP    Ctv--SLG    Ctv--SnF    Ctv--SenBas LaV--Lig   \n[13] LaV--PtC    LaV--SnE    LaV--SnF    LaV--TsS    Lig--PtC    Lig--SnI   \n[19] Lig--StR    Lig--TsS    PtC--SnE    PtC--StR    PtC--TsS    PtC--SenBas\n[25] PtP--SnF    PtP--SnI    PtP--SenBas SLG--SnF    SLG--SnI    SnE--StR   \n[31] SnE--TsS    SnF--SnI    SnI--StR    StR--TsS    StR--SenBas CP --Seri  \n[37] CP --SG     CP --SN     CP --TS     LF --PL     LF --SG     LF --SI    \n[43] PL --SenBas PL --SG     PL --SI     PL --SN    \n+ ... omitted several edges\nWe can associate data with the nodes using the decorate_graph() function. This takes a data.frame object and tries to match up the columns of data in the data.frame to the nodes. Here is an example with some addition built-in data. The option stratum indicates the name of the column that has the node labels in it (which are stored as V(graph)$name).\ndata(baja)\nsummary(baja)\n\n    Region     Population    Latitude       Longitude     \n Baja  :16   BaC    : 1   Min.   :22.93   Min.   :-114.7  \n Sonora:13   Cabo   : 1   1st Qu.:24.45   1st Qu.:-112.6  \n             CP     : 1   Median :27.95   Median :-111.8  \n             Ctv    : 1   Mean   :27.33   Mean   :-111.8  \n             ELR    : 1   3rd Qu.:29.59   3rd Qu.:-110.7  \n             IC     : 1   Max.   :31.95   Max.   :-109.5  \n             (Other):23                                   \n\nlopho &lt;- decorate_graph( lopho, baja, stratum=\"Population\")\nlopho\n\nIGRAPH 6af7beb UNW- 21 52 -- \n+ attr: name (v/c), size (v/n), color (v/c), Region (v/n), Latitude\n| (v/n), Longitude (v/n), weight (e/n)\n+ edges from 6af7beb (vertex names):\n [1] BaC--LaV    BaC--Lig    BaC--PtC    BaC--PtP    BaC--SnE    BaC--SnI   \n [7] BaC--StR    Ctv--PtP    Ctv--SLG    Ctv--SnF    Ctv--SenBas LaV--Lig   \n[13] LaV--PtC    LaV--SnE    LaV--SnF    LaV--TsS    Lig--PtC    Lig--SnI   \n[19] Lig--StR    Lig--TsS    PtC--SnE    PtC--StR    PtC--TsS    PtC--SenBas\n[25] PtP--SnF    PtP--SnI    PtP--SenBas SLG--SnF    SLG--SnI    SnE--StR   \n[31] SnE--TsS    SnF--SnI    SnI--StR    StR--TsS    StR--SenBas CP --Seri  \n[37] CP --SG     CP --SN     CP --TS     LF --PL     LF --SG     LF --SI    \n+ ... omitted several edges\nEach vertex now has seveal different types of data associated with it now. We will use this below.",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Population Graphs</span>"
    ]
  },
  {
    "objectID": "population_graphs.html#adding-data-to-a-graph",
    "href": "population_graphs.html#adding-data-to-a-graph",
    "title": "\n28  Population Graphs\n",
    "section": "",
    "text": "28.1.1 Plotting a graph using ggplot2 routines\nThe ggplot2 package provides a spectacular plotting environment in an intuitive context and there are now some functions to support the Population Graphs in this context.\nIf you haven’t used ggplot2 before, it may at first be a bit odd because it deviates from normal plotting approaches where you just shove a bunch of arguments into a single plotting function. In ggplot, you build a graphic in the same way you build a regression equation. A regression equation has an intercept and potentially a bunch of independent terms. This is exactly how ggplot builds plots, by adding togther components.\nTo specifiy how things look in a plot, you need to specify an aesthetic using the aes() funciton. Here is where you supply the variable names you use for coordinate, coloring, shape, etc. For both of the geom_*set funcitons, these names must be attributes of either the node or edge sets in the graph itself.\nHere is an example using the Lopohcereus graph. We begin by making a ggplot() object and then adding to it a geom_ object. The 5popgraph package comes with two funcitons, one for edges and one for nodes.\n\nlibrary(ggplot2)\np &lt;- ggplot() \np &lt;- p + geom_edgeset( aes(x=Longitude,y=Latitude), lopho ) \np\n\n\n\n\n\n\n\nI broke up the plotting into several lines to improve readability, it is not necessary to to this in practice though. The addition of additional geom_ objects to the plot will layer them on top (n.b., I also passed the size=4 option to the plot as the default point size is a bit too small and this is how you could change that).\n\np &lt;- p +  geom_nodeset( aes(x=Longitude, y=Latitude), lopho, size=4)\np\n\n\n\n\n\n\n\nAnd then you can add additional options to the plot, like axis labels and a less exciting background theme (the theme_empty() provided by popgraph is actually transparent so you can save the image and throw it into a presentation as necessary).\n\np &lt;- ggplot() + geom_edgeset( aes(x=Longitude,y=Latitude), lopho, color=\"darkgrey\" )\np &lt;- p + geom_nodeset( aes(x=Longitude, y=Latitude, color=Region, size=size), lopho) \np &lt;- p + xlab(\"Longitude\") + ylab(\"Latitude\") \np \n\n\n\n\n\n\n\nYou can also use the default layout routines in igraph for visualization. Here is an example using Fruchterman-Reingold algorithm.\n\nlibrary(igraph)\nc &lt;- layout.fruchterman.reingold( lopho )\nV(lopho)$x &lt;- c[,1]\nV(lopho)$y &lt;- c[,2]\np &lt;- ggplot() + geom_edgeset( aes(x,y), lopho, color=\"darkgrey\" )\np &lt;- p + geom_nodeset( aes(x, y, color=Region, size=size), lopho) \np \n\n\n\n\n\n\n\n\n28.1.2 Reading Existing popgraph Files\nThe online versions of Population Graphs provides a *.pgraph file for visualization. These files are visualized in several different software platforms including GeneticStudio (Dyer 2009), a OpenGL visualization application (Dyer & Nason 2004), an online visualization framework at http://dyerlab.org, and of course, in R. We shall focus on this last one. Reading in files to R\n\ngraph &lt;- read.popgraph( \"thegraph.pgraph\" )\n\n\n28.1.3 Saving Population Graph Objects\nA popgraph object is a normal R object and can be saved using the normal R mechanisms.\n\nsave( lopho, file=\"MyLophoGraph.rda\")\n\nFor interoperability, popgraph objects can also be saved in other formats. These are accessed through the write.popgraph() function.\n\nwrite.popgraph(lopho,file=\"~/Desktop/Cactus.pgraph\", format=\"pgraph\")\n\nThere are several other options available for outputing your graph. Currently the other formats that have been implemented are:\n\n\njson A format for html javascript data processing.\n\n\n\n\nkml The Keyhole Markup Language which is read by GoogleEarth. This requires Latitude and Longtitude vertex properties.\n\ngraphml The graph markup language.\n\nhtml Export as an interactive html document you can manipulate on your desktop (uses javascript d3.js library so you need an internet connection).\n\npajek Export to a format that works with the software Pajek (http://pajek.imfm.si/doku.php?id=pajek)\n\npgraph The format used in GeneticStudio and the original popgraph 3D viewer (this is the default).\n\nadjacency Saves the adjacency matrix of the graph (binary) as a CSV file\n\npaths Saves the shortest paths matrix as a CSV file\n\nweights Saves the adjacency matrix with edge weights.",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Population Graphs</span>"
    ]
  },
  {
    "objectID": "population_graphs.html#interactive-networks",
    "href": "population_graphs.html#interactive-networks",
    "title": "\n28  Population Graphs\n",
    "section": "\n28.2 Interactive Networks",
    "text": "28.2 Interactive Networks\nTo create a popgraph, you need to pass the popgraph() function genotypes as multivariate variables—the function to_mv() does this behind the scene—and a vector of variables that allocate each row of data to a node. Here we use the ‘Population’ vector from the arapat data.frame.\n\nlibrary(gstudio)\nlibrary(popgraph)\ndata(arapat)\ngraph &lt;- popgraph(to_mv(arapat),groups = arapat$Population)\nprint(graph)\n\nIGRAPH 0e03647 UNW- 39 71 -- \n+ attr: name (v/c), size (v/n), weight (e/n)\n+ edges from 0e03647 (vertex names):\n [1] 101--102   101--32    102--32    12 --161   12 --165   12 --93   \n [7] 153--165   153--58    156--157   156--48    156--73    156--75   \n[13] 157--48    157--Aqu   157--ESan  159--171   159--173   159--89   \n[19] 160--168   160--169   160--93    160--SFr   161--162   161--165  \n[25] 161--93    161--SFr   162--64    162--77    162--93    163--75   \n[31] 163--Const 163--ESan  164--165   164--169   164--51    164--Const\n[37] 164--SFr   165--168   165--169   165--77    166--168   168--51   \n[43] 168--58    168--64    168--77    169--58    169--93    169--SFr  \n+ ... omitted several edges\n\n\nThe forceNetwork() function is what does the plotting and it needs some data that are in a specific format. Essentially, there needs to be two data.frame objects with the following attributes:\n\n\nnodes - A data.frame with each row representing the name of the node to be displayed, the group the node belongs to (if there are groupings of nodes to be displayed by alternative colors), and a vector of node sizes.\n\n\nedges - A data.frame representing the edge connecting the nodes, labeled as ‘from’ and ‘to’ and a vector of weights.\n\nThe ‘from’ and ‘to’ vectors need to be numeric values of the nodes in the other data frame and need to be 0-indexed (e.g., the first node name it is going to look up is indexed as ‘0’ in the javascript instead of ‘1’ as is common in R).\n\nnodes &lt;- to_data.frame( graph, mode=\"nodes\", as.named=FALSE )\nedges &lt;- to_data.frame( graph, mode=\"edges\", as.named=FALSE )\nedges$source &lt;- edges$source - 1\nedges$target &lt;- edges$target - 1 \n\nThe only last thing to do is to define a grouping of populations. This will be represented in the network as a coloring. For this one, I’m going to use the hypothesized STRUCTURE clustering (see @ref(admixture) for how this was done). In the arapat data set, there is a designation for each individual on which cluster they belong. Some populations are ‘pure’ in their groupings but others (in spatial regions of sympatry) they are mixed. Below I determine the estimated STRUCTURE groups for each population and collapse those who have more than one into a single string.\n\ngrps &lt;- by( arapat$Cluster, arapat$Population, unique )\nl &lt;- lapply( grps, function(x) { g &lt;- paste(sort(x),collapse=\"/\") })\ndf &lt;- data.frame( name=names(l), group=as.character(l))\nnodes &lt;- merge( nodes, df )\n\nOnce defined, we can then call the function to make the data.frame objects and then do the plot. These graphics are interactive, grab a node and drag it around!\n\nlibrary(networkD3)\nforceNetwork(Links = edges, Nodes = nodes,\n            Source = \"source\", Target = \"target\",\n            Value = \"value\", NodeID = \"name\",\n            Group = \"group\", opacity=1.0,\n            legend=TRUE, fontSize = 16,\n            zoom=TRUE )",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Population Graphs</span>"
    ]
  },
  {
    "objectID": "population_graphs.html#spatial-population-graphs",
    "href": "population_graphs.html#spatial-population-graphs",
    "title": "\n28  Population Graphs\n",
    "section": "\n28.3 Spatial Population Graphs",
    "text": "28.3 Spatial Population Graphs\nMapping the nodes and edges onto real space is a key task in the understanding of how covariance is partitioned on the landscape. There are several approaches that can be used in R since it is such a flexible platform. In what follows I will use a series of techniques that I find useful ordered from the simplest to the more complicated.\n\n28.3.1 Integrating Google and ggplot2 for Plotting\nR has some pretty good facilities for using spatial assests from Google and OpenStreetMaps and is a very easy way to get quick plots from Population Graphs, particularly if you can integrate it into the ggplot2 framework.\nUsing the ggmap package, you can request map tiles and use as backgrounds onto which you can plot specific objects. To do so, you must first get:\n\nEither the centroid of the location you are interested in finding and a value of zoom (just like in google maps), or\nA bounding box with left, bottom, right, top coordinates. This is a bit of an experimental thing and does not always get you what you want.\n\nSome fiddling is required with either way you go. The map you get from get_map() is essentially a huge matrix of hex colors as shown above.\n\nlibrary(ggmap)\nlocation &lt;- c( mean(V(lopho)$Longitude), mean(V(lopho)$Latitude))\nlocation\nmap &lt;- get_map(location,maptype=\"satellite\", zoom=6)\ndim(map)\nmap[1:4,1:4]\n\nThis map object can be passed on to ggmap(), which replaces the traditional ggplot() funciton and sets up the bounding box in terms of Latitude and Longtidue. Onto this, you can plot the graph topologoy using:\n\n\ngeom_edgeset() This takes the graph and plots out the edges.\n\n\ngeom_nodeset() This plots out the nodes. You could probably use a regular data.frame and geom_point() as well. Here is an example:\n\n\np &lt;- ggmap( map ) \np &lt;- p + geom_edgeset( aes(x=Longitude,y=Latitude), lopho, color=\"white\" ) \np &lt;- p + geom_nodeset( aes(x=Longitude, y=Latitude, color=Region, size=size), lopho) \np + xlab(\"Longitude\") + ylab(\"Latitude\")\n\n\n28.3.2 Integrating Raster Maps\nAt times we have raster data upon we can plot a population graph. Here is an example from Baja California. The underlying raster image is croped from a WorldClim tile and represents elevation.\n\nlibrary(raster)\ndata(alt)\nplot(alt)\n\n\n\n\n\n\n\nSince it is a raster object, it knows how to plot itself relatively well. There are a ton of good references for showing you how to play with raster data (e.g., Bivand et al. 2008)\nTo plot our graph onto this topology, we export the spatial components of the graph into objects that interact with rasters. The packages provides simple extraction of features into SpatialLines and SpatialPoints objects.\n\nlopho.nodes &lt;- to_SpatialPoints(lopho)\nlopho.nodes\n\nclass       : SpatialPoints \nfeatures    : 21 \nextent      : -114.73, -109.99, 23.58, 31.95  (xmin, xmax, ymin, ymax)\ncrs         : NA \n\nlopho.edges &lt;- to_SpatialLines(lopho)\nhead(lopho.edges)\n\nclass       : SpatialLines \nfeatures    : 1 \nextent      : -111.79, -109.99, 24.04, 26.59  (xmin, xmax, ymin, ymax)\ncrs         : NA \n\n\nOnce we have them extracted into the right format, we can add them to the raster plot. I plot the nodes twice to overlay a circular icon (pch=16) onto the default cross marker and make them 50 % larger (cex=1.5).\n\nplot( alt )\nplot( lopho.edges, add=TRUE, col=\"#555555\" )\nplot( lopho.nodes, add=TRUE, col=\"black\", cex=1.5 )\nplot( lopho.nodes, add=TRUE, col=V(lopho)$color, pch=16, cex=1.5 )",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Population Graphs</span>"
    ]
  },
  {
    "objectID": "population_graphs.html#extracting-spatial-data-using-population-graphs",
    "href": "population_graphs.html#extracting-spatial-data-using-population-graphs",
    "title": "\n28  Population Graphs\n",
    "section": "\n28.4 Extracting Spatial Data Using Population Graphs",
    "text": "28.4 Extracting Spatial Data Using Population Graphs\nSince we are dealing with the spatial stuff right now, it makes sense to look into how we can use the topological features of the graph to extract spatial data.\n\n28.4.1 Node Specific Data\nThe node data nodes is a SpatialPoints object and can be used to pull data from raster sources. I’ll start by creating a data.frame with some existing data in it.\n\ndf.nodes &lt;- data.frame(Pop=V(lopho)$name, Latitude=V(lopho)$Latitude, Longitude=V(lopho)$Longitude)\n\nThen we can extract the elevation from the alt raster as:\n\nlibrary(raster)\ndf.nodes$Elevation &lt;- extract( alt, lopho.nodes )\nsummary(df.nodes)\n\n     Pop               Latitude       Longitude        Elevation    \n Length:21          Min.   :23.58   Min.   :-114.7   Min.   :  5.0  \n Class :character   1st Qu.:25.73   1st Qu.:-112.9   1st Qu.: 14.0  \n Mode  :character   Median :28.82   Median :-112.0   Median : 66.0  \n                    Mean   :27.91   Mean   :-112.2   Mean   :159.2  \n                    3rd Qu.:29.73   3rd Qu.:-111.3   3rd Qu.:259.0  \n                    Max.   :31.95   Max.   :-110.0   Max.   :667.0  \n\n\nAdditional data could be extracted from other rasters. See http://worldclim.org for some example data that may prove useful.\n\n28.4.2 Extracting Data Along Popgraph Edges\nIt is also possible to extract data along vectors, or other SpatialLines objects, which the edges in a popgraph can be tranformed into. This is a particularly helpful approach if you are trying to quantify the value of characteristics of the environment between your sampling locations. In the following example, I estimate the popualtion graph from the arapat data set\n\ngraph &lt;- popgraph( to_mv( arapat ), arapat$Population )\ncoords &lt;- strata_coordinates(arapat)\ngraph &lt;- decorate_graph( graph, coords )\nedges &lt;- to_SpatialLines(graph)\nproj4string(edges) &lt;- CRS( proj4string( alt ))\nplot( alt, legend=FALSE)\nplot(edges,add=TRUE)\n\n\n\n\n\n\n\nand determine which of the edges has the longest length.\n\nedge_lengths &lt;- SpatialLinesLengths( edges )\nlongest &lt;- sort( edge_lengths,decreasing = TRUE )[1]\nlongest\n\nEdge 164 SFr \n    322.8491 \n\n\nThis edge is found at:\n\nidx &lt;- which( edge_lengths == longest )\nidx\n\nEdge 164 SFr \n          37 \n\nedge &lt;- edges[ 37 ]\nedge\n\nclass       : SpatialLines \nfeatures    : 1 \nextent      : -112.964, -111.5441, 24.74642, 27.3632  (xmin, xmax, ymin, ymax)\ncrs         : +proj=longlat +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +no_defs \n\nplot( alt, legend=FALSE)\nplot(edge,add=TRUE)\n\n\n\n\n\n\n\nFrom this edge object (or a collection of edge objects) we can again extract values from a raster. Here I pull out the elevation profile of this edge. To plot it, I need to make a sequence of latitude values equal in length to that of the observed elevation values I extracted.\n\nelev &lt;- extract( alt, edge )[[1]]\nfrom_lat &lt;- V(graph)$Latitude[V(graph)$name == \"SFr\"]\nto_lat &lt;- V(graph)$Latitude[ V(graph)$name == \"164\"]\nlat &lt;- seq( from=from_lat, to=to_lat, length.out = length(elev) )\ndf &lt;- data.frame( Latitude=lat, Elevation=elev)\np &lt;- ggplot( df, aes(Latitude,Elevation)) + geom_line(color=\"lightgrey\") \np + geom_point() + ylab(\"Elevation (m)\")\n\n\n\n\n\n\n\nPopulation Graphs have been used to determine if there is an preference (or avoidance) of a specific raster value for the location of individual edges on the landscape. For example, Dyer et al. (2012) were interested in determining if there the edge in the pollination graph (e.g., a population graph constructed from male pollen haplotypes) preferentially traverse (or avoid) specific intervening habitat features. To do this, they permuted the edge set among nodes in the graph and recorded the prevalence (mean and variance) of specific features extracted from specific categorical rasters representing both canopy and understory features. The permutation of a network should be done such that you preserve aspects of the spatial heterogeneity and spatial arrangement of the nodes on the landscape. You probably do not want to permute all edges randomly in the graph (though this option is available in the code), but would probably be better served by permuting the network while maintaining both the node coordinates (e.g., where they exist on the landscape) as well as the degree distribution of the overall network. This second criteria holds constant higher network structure. The general idea is to:\n\nCreate a saturated graph and extract ecological features for all potential connections. This gives us a data.frame within which we can pull out ecological values for each permutation. This is the most computationally intensive process and doing it once and then extracting values from the data.frame for each permutation is a more efficient approach.\nOnce you have all the potential values of your features, you can permute the observed matrix, while holding both the connection probability (e.g., the number of edges) and the degree distribution (e.g., the amount of edges connected to nodes) constant using the randomize_graph function included in the popgraph library. For each permutation, you then compile the permuted environmental factor as a null distribution and then compare those to the observed.\n\nThis may sound a bit convoluted, but this example may help. Consider the hypothetical case where we think that the edges in the population graph from Figure @ref(fig:popgraphInBaja), are restricted in elevation because we believe that dispersing insects fly around high elevation mountains rather than over them.1 If this is true, then we should expect that the average (or max) elevation along any of the observed edges in the Population Graph would be less than what would be expected if we permuted the edges among nodes and measured elevation along edges from permuted graphs.\nFirst, we need to set up the network and extract values of elevation along all potential edges. I make a saturated graph from all potential\n\ndata(baja)\ngraph &lt;- popgraph( to_mv( arapat ), arapat$Population )\ngraph &lt;- decorate_graph( graph, coords )\nallpops &lt;- V(graph)$name\n\nI then can make an adjacency matrix connecting all pairs of populations\n\nA &lt;- matrix(1,nrow=length(allpops),ncol=length(allpops))\ndiag(A) &lt;- 0\nrownames(A) &lt;- colnames(A) &lt;- allpops\nsaturated_graph &lt;- graph.adjacency(A, mode = \"undirected\")\nsaturated_graph &lt;- as.popgraph( saturated_graph )\n\nFrom which I can pull all the edges (all 741 of them) as SpatialLines objects\n\nsaturated_graph &lt;- decorate_graph( saturated_graph, coords )\nall_edges &lt;- to_SpatialLines( saturated_graph )\n\nFrom these 741 SpatialLines objects, we can extract data from the elevation raster.\n\nedge_values &lt;- extract( alt, all_edges, fun=max, na.rm=TRUE, df=TRUE)\n\nThis will take a bit of time to complete. The options that I provided were:\n- fun=max - The function used is the max function. - na.rm=TRUE - Ignore all missing data (e.g., when an edge crosses water on the alt raster, the extracted values are NA) - df=TRUE - Return the answer as a data.frame object instead of just a vector.\n\nload(\"./spatial_data/edge_values.rmd\")\n\nThis data.frame has two columns, one for edge number and the other for value. I’m going to put an additional pair of columns with the names of the nodes the edges are connected to into this data.frame\n\nedge_names &lt;- as_edgelist( saturated_graph )\nedge_values$Nodes &lt;- paste( edge_names[,1], edge_names[,2], sep=\"-\")\nhead(edge_values)\n\n  ID alt_22   Nodes\n1  1     25 101-102\n2  2    906  101-12\n3  3     31 101-153\n4  4     72 101-156\n5  5   1079 101-157\n6  6   1361 101-159\n\n\nThis constitutes all potential connections across the landscape. From this we can extract the edges that we observed in the original Population Graph\n\ne &lt;- as_edgelist( graph )\nobs &lt;- edge_values$alt_22[ edge_values$Nodes %in% paste( e[,1], e[,2], sep=\"-\") ]\nmean(obs)\n\n[1] 744.5634\n\n\nWe can now permute the network a moderate number of times and take the values of permuted elevation to see if our observed are smaller than all potential elevations for this specific network.\n\nperm_elev &lt;- rep(NA,999)\nfor( i in 1:length(perm_elev) ) {\n  perm_graph &lt;- randomize_graph( graph )\n  e &lt;- as_edgelist( perm_graph )\n  perm_val &lt;- edge_values$alt_22[ edge_values$Nodes %in% paste( e[,1], e[,2], sep=\"-\") ]\n  perm_elev[i] &lt;- mean(perm_val)\n}\n\nNow, we can see where the observed value occurs in the distribution of elevations created under the null hypothesis of no difference in elevation across edges.\n\ndf &lt;- data.frame( Elevation=c(mean(obs),perm_elev), Category=c(\"Observed\",rep(\"Permuted\",999)))\nggplot( df, aes(x=Elevation,fill=Category)) + geom_histogram(stat=\"bin\", bins=40) + xlab(\"Elevation (m)\") + ylab(\"Distribution of Permuted Elevations\")\n\n\n\n\n\n\n\nIn fact, we can estimate the probablity as:\n\nsum( mean(obs) &gt;= perm_elev )\n\n[1] 0\n\n\nAs it turns out, the observed edges do in fact appear to be traversing lower elevations than the potential set of edges that could be present (while controlling for spatial location of populations and graph structure).",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Population Graphs</span>"
    ]
  },
  {
    "objectID": "population_graphs.html#extracting-graph-theoretic-parameters",
    "href": "population_graphs.html#extracting-graph-theoretic-parameters",
    "title": "\n28  Population Graphs\n",
    "section": "\n28.5 Extracting Graph-Theoretic Parameters",
    "text": "28.5 Extracting Graph-Theoretic Parameters\nThe underlying structure of a popgraph object is based upon the igraph package from Gabor Csardi. A population graph is essentially a specific kind of igraphobject and can be decorated with metadata that is useful for spatial population genetic analyses. As such, there is a wealth of existing analyses from both the igraph as well as the sna packages that can be used on popgraph objects. Here are some examples.\n\n28.5.1 Matrix Representations of Population Graph Objects\nA graph topology is a graphical representation of a matrix and there are several reasons why you may want to use these matrices. The function to_matrix() is an easy front-end to several kinds of matrices. Matrix structure itself can be defined by adjacency matrics, either binary (the default) or weighed by the edge weight. Several graph-theoretic parameters are derived from the adjacency matrix. Here is an example from our little graph that started this document.\n\nto_matrix( lopho, mode=\"adjacency\")[1:5,1:5]\n\n    BaC Ctv LaV Lig PtC\nBaC   0   0   1   1   1\nCtv   0   0   0   0   0\nLaV   1   0   0   1   1\nLig   1   0   1   0   1\nPtC   1   0   1   1   0\n\nto_matrix( lopho, mode=\"edge weight\")[1:5,1:5]\n\n          BaC Ctv       LaV      Lig      PtC\nBaC  0.000000   0  9.052676  9.71615 12.38248\nCtv  0.000000   0  0.000000  0.00000  0.00000\nLaV  9.052676   0  0.000000 12.07282 12.80017\nLig  9.716150   0 12.072820  0.00000 14.22483\nPtC 12.382480   0 12.800170 14.22483  0.00000\n\n\nIn addition to who each node is connected to, it is often of interest to know the length of the shortest path through the matrix connecting nodes. Here is a slightly larger example, using the cactus data so we can look at isolation by graph distance.\n\ncGD &lt;- to_matrix( lopho, mode=\"shortest path\")\ncGD[1:5,1:5]\n\n          BaC       Ctv       LaV      Lig      PtC\nBaC  0.000000  9.195038  9.052676  9.71615 12.38248\nCtv  9.195038  0.000000 13.083311 15.23302 20.49099\nLaV  9.052676 13.083311  0.000000 12.07282 12.80017\nLig  9.716150 15.233023 12.072820  0.00000 14.22483\nPtC 12.382480 20.490990 12.800170 14.22483  0.00000\n\n\nIt should be noted that the shortest distance through a population graph is defined as the parameter \\(cGD\\), conditional graph distance (see Dyer et al. 2010 for more information on this parameter).\nNow, we need the physical distance between the nodes. If the physical size of the sampling area is small we could just use the Pythagorean equation. However, here the distance is relatively large and the curvature of the earth may be of interest to take into account. There are seveal functions that will calculate ‘great circle distance’ but the easiest is rdist.earth() from the fields funtion.\n\nlibrary(gstudio)\ndf &lt;- data.frame( Stratum = V(lopho)$name, \n                  Longitude  =V(lopho)$Longitude, \n                  Latitude = V(lopho)$Latitude ) \npDist &lt;- strata_distance( df )\n\nNow, we can plot these values against each other to see if there is a pattern of ‘isolation by distance’ captured in the graph topology. To do this, I extract only the upper triangle (e.g., the values above the diagonal of each matrix) because they are symmetric matrices and we do not want to look at each datum twice.\n\ndf &lt;- data.frame( cGD=cGD[upper.tri(cGD)], Phys=pDist[upper.tri(pDist)])\ncor.test( df$Phys, df$cGD, method=\"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  df$Phys and df$cGD\nS = 729992, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.5270434 \n\n\nWe can plot these values and make a trendline pretty easily. Here is a plot using ggplot2 (a very nice plotting library; you could use plot() to do the normal plotting but I think ggplot2 does such a nice job I encourage its use).\n\nqplot( Phys, cGD, geom=\"point\", data=df) + stat_smooth(method=\"loess\") + xlab(\"Physical Distance\") + ylab(\"Conditional Genetic Distance\")\n\n\n\n\n\n\n\nThe trendline is the loess predicted line with confidence interval.\n\n28.5.2 Node Specific Parameters\nFeatures of the topology can be extracted as either properties of the nodes or the edges. Node properties may provide insights into localized processes (e.g., a ‘sink’ population). There are a lot of different parameters that can be derived and several packages in R that help out. Here are some basic ones.\n\ndf.nodes$closeness &lt;- closeness(lopho)\ndf.nodes$betweenness &lt;- betweenness(lopho)\ndf.nodes$degree &lt;- degree( lopho )\ndf.nodes$eigenCent &lt;- evcent( lopho )$vector\ndf.nodes$Region &lt;- factor(V(lopho)$Region)\nsummary(df.nodes,color=\"Region\")\n\n     Pop               Latitude       Longitude        Elevation    \n Length:21          Min.   :23.58   Min.   :-114.7   Min.   :  5.0  \n Class :character   1st Qu.:25.73   1st Qu.:-112.9   1st Qu.: 14.0  \n Mode  :character   Median :28.82   Median :-112.0   Median : 66.0  \n                    Mean   :27.91   Mean   :-112.2   Mean   :159.2  \n                    3rd Qu.:29.73   3rd Qu.:-111.3   3rd Qu.:259.0  \n                    Max.   :31.95   Max.   :-110.0   Max.   :667.0  \n   closeness         betweenness        degree        eigenCent         Region\n Min.   :0.002202   Min.   : 0.00   Min.   :3.000   Min.   :0.0005975   1:12  \n 1st Qu.:0.002466   1st Qu.: 0.00   1st Qu.:4.000   1st Qu.:0.0040621   2: 9  \n Median :0.002919   Median : 4.00   Median :5.000   Median :0.1514601         \n Mean   :0.002891   Mean   :17.43   Mean   :4.952   Mean   :0.3043499         \n 3rd Qu.:0.003227   3rd Qu.:25.00   3rd Qu.:6.000   3rd Qu.:0.6687298         \n Max.   :0.003844   Max.   :98.00   Max.   :7.000   Max.   :1.0000000         \n\n\nThe relationship betwwen the node variables can be evaluated in a pair plot.\n\nlibrary(GGally)\nggpairs(df.nodes,columns=2:9, color='Region')\n\n\n\n\n\n\n\n\n28.5.3 Edge Specific Parameters\nEdges may have specific properties as well. Here are some examples using betweeness centrality, community, and regionality (if the edge connects within Baja or Sonora or crosses the Sea of Cortéz).\n\ndf.edge &lt;- data.frame( Weight=E(lopho)$weight )\ndf.edge$betweenness &lt;- edge.betweenness(lopho)\ndf.edge$Region &lt;- rep(\"Baja\",52)\ndf.edge$Region[36:52] &lt;- \"Sonora\"\ndf.edge$Region[c(11,24,27,35)] &lt;- \"Cortez\"\nggpairs(df.edge, color=\"Region\")",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Population Graphs</span>"
    ]
  },
  {
    "objectID": "population_graphs.html#testing-for-topological-congruence",
    "href": "population_graphs.html#testing-for-topological-congruence",
    "title": "\n28  Population Graphs\n",
    "section": "\n28.6 Testing for Topological Congruence",
    "text": "28.6 Testing for Topological Congruence\nIf you have more than one topology and the node sets overlap sufficiently, you can test for the topological congruence of the two. There are several specific reasons why population graph topologies may be congruent. In this section we use the spatial genetic structure of the Senita cactus (Lophocereus schottii) and its obligate pollinator, Upiga virescens as an example. This is an obligate pollination mutualism and as such we should expect there to be some degree of spatial genetic congruence between the species due to this co-evolution.\n\ndata(upiga)\nupiga &lt;- decorate_graph(upiga,baja,stratum=\"Population\")\nupiga.nodes &lt;- to_SpatialPoints(upiga)\nupiga.edges &lt;- to_SpatialLines(upiga)\n\nThese two data sets were ‘mostly’ collected in the same physical locations. Here is a comparison of the two topologies.\n\npar(mfrow=c(1,2))\nplot(lopho)\nplot(upiga)\n\n\n\n\n\n\n\nYou can clearly see some differences in both the node and edge sets. However, given the relationship between these organisms, there is an expectation that they should share some spatial structure. The function congruence_topology() is designed to extract the congruence graph that is the intersection of both node and edge sets. If the node sets are not completely overlapping (in this case they are not), it will give you a warning. If you want to compare topologies, you must start with identical node sets because the topology in a Population Graph is based upon the entire structure, not just pairwise differences. See the writeup about the gstudio package for more information on this.\n\ncong &lt;- congruence_topology(lopho,upiga)\nplot(cong)\n\n\n\n\n\n\n\nWe can then take the congruence graph and plot it or work with it in the normal fashion.\n\ncong &lt;- decorate_graph( cong, baja )\ncong.nodes &lt;- to_SpatialPoints(cong)\ncong.edges &lt;- to_SpatialLines(cong)\nplot(alt)\nplot(cong.edges,add=T)\nplot(cong.nodes,add=T, pch=16, col=\"red\")\n\n\n\n\n\n\n\nThere are several ways to examine ‘congruence’ in graph topologies, of which I show two. The first method is based upon the correlation of pair-wise distance through the graph for each. That is to say, are proximate nodes in lopho similarily close in upiga? This is called “Distance Congruence” and is based upon a non-parametric correlation of path distances.\n\ntest_congruence(lopho,upiga,method=\"distance\")\n\n\n    Pearson's product-moment correlation\n\ndata:  distances.graph1 and distances.graph2\nt = 7.3025, df = 118, p-value = 3.6e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4207347 0.6701315\nsample estimates:\n      cor \n0.5579029 \n\n\nAnother way is to ask about the pattern of connectivity. Based upon the number of nodes and edges in lopho and upiga, are there more in the congruence graph than could be expected if the two graph were just randomly associated? This is called “Structural Congruence” and is determined combinatorially. What is returned by this is the probability having as many edges as observed in the congruence graph given the size of the edge sets in the predictor graphs. You can think of this as the fraction of the area under the line as large or larger than the observed.\n\ntest_congruence(lopho,upiga, method=\"combinatorial\")\n\n       CDF \n0.03625037 \n\n\nHope this provides enough of an overview of the popgraph package to get you started. If you have any questions feel free to email [mailto://rjdyer@vcu.edu].",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Population Graphs</span>"
    ]
  },
  {
    "objectID": "population_graphs.html#footnotes",
    "href": "population_graphs.html#footnotes",
    "title": "\n28  Population Graphs\n",
    "section": "",
    "text": "This is a truly hypothetical and contrived example to show how you would do this in the code and is not motivated by any biologically motivated reasons. Ya’ gotta make up examples sometimes…↩︎",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Population Graphs</span>"
    ]
  },
  {
    "objectID": "raster_data.html",
    "href": "raster_data.html",
    "title": "\n31  Raster Data\n",
    "section": "",
    "text": "31.1 Cropping Rasters\nRasters are a form of data that is georeferenced and (somewhat) continuous. Raster data is perhaps best envisioned as a matrix of values, whose entries represent spatially referenced data points. The raster itself can be visualized as you could for normal matrix output. What makes a raster different, however, is that it is (or should be) georeferenced. This means that each element of the matrix represents some measurement on the ground having a specific location and spread. This is analogous to an image, where if you zoom in on it enough, you will be able to differentiate between individual pixels, it is just that for rasters, each pixel has a spatial location and size associated with it that we can map onto the earth.\nYou can either create raster objects de novo or you can acquire them from some external source. To create one from scratch, you start with a matrix of values and then construct the raster object using the raster() function as:\nwhich can be visualized using the normal plot command. The raster library has overridden several of the plotting functions and you can plot raster objects and decorate the images in the same way you do for normal plotting materials (@ref(graphics)).\nThere are also many available repositories for raster data including Open Source, Governmental, and Municipal locations. One common source for these data is that of http://worldclim.org. This repository contains temperature and precipitation data generalized for the entire globe.\nThese data are available free of charge and have been used in numerous biological studies. Moreover, they provide a set of ‘biologically relevant’ layers, called BioClim, that summarize both temperature and precipitation. They motivate these by saying:\nThese layers are encoded into 19 Bio-layers as defined in Table @ref(tab:bioclim). These layers are available for download from their site directly (I recommend using the tiles approach so you do not have to download the entire world map) as well as from the R package dismo.\nFor the purposes of this chapter, I’ll use bioclim and altitude layers from tile 22, which encompasses the spatial distribution of sampling locations in Baja California for the Araptus attenuatus dataset\nCommon raster formats include GeoTiff, essentially an image file with some metadata associated with it, and BIL (Binary interleaved) file formats. Both of these types are available from WorldClim. In general, the GeoTiff format is slightly easier to work with as all the data is contained within a single file, whereas the BIL format has two files for each raster (the second file is a header file that has the spatial meta data associated with it). If you do use the BIL format, the file path you pass to raster() would be of the BIL file, not the header one.\nFrom Worldclim, I downloaded the elevation raster for Tile 22 and can load it into R using the raster() function as:\nThe alt object is summarized here. A couple of things should be pointed out here: - In total, this is an object with 12,960,000 entries in it!\n- The resolution of each ‘pixel’ in this representation is 0.008, which is about 30-arc seconds or ~1km. That means that each loation in the study area is represented by the same exact value as the surrounding square kilometer. Obviously, if you are working on processes whose spatial scale is relevant less than 1000 m^2^, this kind of data is going to be of little value to you. - The values within the matrix range from -202 upwards to 5469. This is in meters.\nIn addition, the raster has a spatial extent and a projection associated with it. For more information on projections see @ref(map-projections).\nThis elevation raster looks like:\nJust because we have a large raster does not mean that it is in your best interest to use the entire object. Much of the spatial analyses routines used in population genetics require measurements of intervening distance, either Euclidean or ecological. Many of the routines for estimation of these distances require the estimation of pairwise distance between all pixels. For our purposes, the arapat dataset does not occur throughout most of this map, so it is in our best interests to use only the portion of the raster relevant to our data rather than the entire thing.\nHere is one way of going this. I first define an extent, which consists of a vector representing the coordinates for xmin, xmax, ymin, and ymax (in decimal degrees longitude and latitude). You then crop() the raster to that extent.\ne &lt;- extent( c(-115,-109,22,30) )\nbaja_california &lt;- crop( alt, e )\nplot(baja_california, xlab=\"Longitude\",ylab=\"Latitude\")\nLets make this base map a bit more pretty by taking the altitude and estimating the slope of each pixel and the direction it is facing (aspect). From this, we can ‘shade’ the hills in the map giving it more of a relief view we commonly see in maps.\nslope &lt;- terrain( baja_california, opt=\"slope\" )\naspect &lt;- terrain( baja_california, opt=\"aspect\")\nbaja &lt;- hillShade( slope, aspect, 40, 270 )\nplot(baja, xlab=\"Longitude\",ylab=\"Latitude\", legend=FALSE)\nOnto this map, we can plot our populations. For this, we convert the raw coordinates into a SpatialPoints object (see @ref(vector-data)) and then overlay onto the map. I use two points() commands to make the symbol for each population.\nlibrary(gstudio)\ndata(arapat)\ncoords &lt;- strata_coordinates(arapat)\npts &lt;- SpatialPoints( coords[,2:3], proj4string = CRS(proj4string(baja)))\nplot(baja, xlab=\"Longitude\",ylab=\"Latitude\", legend=FALSE)\npoints( pts, col=\"darkred\", pch=3)\npoints( pts, col=\"red\", pch=16)",
    "crumbs": [
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "raster_data.html#cropping-rasters",
    "href": "raster_data.html#cropping-rasters",
    "title": "\n31  Raster Data\n",
    "section": "",
    "text": "31.1.1 Cropping Rasters Via Polygons\nIt is also possible to crop a raster with a more fine grained approach using a polygon. Here is an example using five points picked around the region of Loreto, BCS (I just grabbed these by looking at Google Earth). You define a polygon by a series of points, the last of which has to be identical to the first one so that the polygon is a closed object and not just a series of points on a crooked line…\n\npts &lt;- rbind( c(-111.5,27.0),\n              c(-112.4,26.7),\n              c(-111.7,25.7),\n              c(-111.1,25.4),\n              c(-110.8,26.0),\n              c(-111.5,27.0) )\npts\n\n       [,1] [,2]\n[1,] -111.5 27.0\n[2,] -112.4 26.7\n[3,] -111.7 25.7\n[4,] -111.1 25.4\n[5,] -110.8 26.0\n[6,] -111.5 27.0\n\n\nFrom these points, we construct a SpatialPolygons object (see @ref{polygons} for more info on this convoluted construction) and then can overlay onto the map to make sure it in the correct vicinity (here we are eyeballing it a bit). For more on why this next line of code looks so crazy, see @ref(polygons).\n\npolys &lt;- SpatialPolygons(list(Polygons(list(Polygon(pts)),\"Polygon\")))\nplot(baja, legend=FALSE)\nplot(polys, add=TRUE)\n\n\n\n\n\n\n\nTo use the polygon to crop the raster, we have to both remove the part of the raster that is not contained within the polygon (mask) and then cut down the remaining raster to change the bounding box to that representing the portion of the data that remains (trim). If you do not trim the raster, it will have the same amount of data associated with it as the previous raster (e.g., the underlying data matrix will have 960 rows and 720 columns) but the part that is masked will be represented by NA values. For many rasters, the data is held in-memory (see the entry for ’data sourcein the summary above) and as such removing as much of a raster that isNA` improves your ability to manipulate it better.\n\nloredo &lt;- trim( mask( baja, polys ) )\nplot(loredo)\n\n\n\nExtraction of\n\n\n\n\n31.1.2 Cropping Rasters Via Convex Hull\nAn analysis common to modern population genetics is that of finding ecological distances between objects on a landscape. The estimation of pairwise distance derived from spatial data is a computationally intensive thing, one that if you are not careful will bring your laptop to its knees! One way to mitigate this data problem is to use a minimal amount raster area so that the estimation of the underlying distance graph can be done on a smaller set of points.\nCropping by a polygon like demonstrated in the previous example is a ‘by hand’ approach to estimating a box that roughly encompasses your data points. A more efficient one is one where you simply provide your coordinates and we can estimate a polygon that surrounds those coordinates with the minimal amount of wasted space. This is called a Convex Hull, which is kind of like a polygon that is created as if there was a rubbrerband fit around all your points. It is a minimal area that includes all of your points.\nFor this example, I’m going to use the populations found along the peninsula and find the minimal area encompassing those points.\n\nlibrary(gstudio)\ndata(arapat)\ncoords &lt;- strata_coordinates(arapat)\nbaja_coords &lt;- coords[ !(coords$Stratum %in% c(\"101\",\"102\",\"32\")), ]\nbaja_pts &lt;- SpatialPoints( baja_coords[,2:3])\nplot(baja, legend=FALSE)\nplot(baja_pts,add=T,col=\"darkred\")\nplot(baja_pts,add=T,col=\"darkred\",pch=16)\n\n\n\n\n\n\n\nThe methods for finding the hull and adding a buffer around it are found in the rgeos package. These are pretty easy functions to use and are very helpful. If you are having trouble installing the rgeos package from source, see @ref(rgdal-rgeos-packages).\n\nlibrary(rgeos) # loads in gConvexHull & gBuffer functions\nhull &lt;- gConvexHull(baja_pts)\nplot(baja, legend=FALSE)\nplot(baja_pts,add=T,col=\"darkred\")\nplot(baja_pts,add=T,col=\"darkred\",pch=16)\nplot(hull,add=T,border=\"red\")\n\nThe function gConvexHull() returns an object of type SpatialPolygons, just like we created before. However, we now have a polygon that has each of our most ‘outward’ populations on the very edge of the polygon. It may be beneficial for us to add a buffer around this polygon.\n\nhull_plus_buffer &lt;- gBuffer(hull,width=.2)\nplot(baja, legend=FALSE)\nplot(baja_pts,add=T,col=\"darkred\")\nplot(baja_pts,add=T,col=\"darkred\",pch=16)\nplot(hull_plus_buffer, add=T, border=\"red\")\n\nNow, we can mask and trim it to include only the area of interest.\n\npop_hull &lt;- trim( mask(baja,hull_plus_buffer) )\nplot(pop_hull, legend=FALSE, xlab=\"Longitude\", ylab=\"Latitude\")\nplot(baja_pts,add=T,col=\"darkred\",pch=16)\n\nThis would be a great raster to start looking at ecological separation in since we have removed the extraneous data that would unintentionally cause problems with the distance estimators.",
    "crumbs": [
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "raster_data.html#modifying-rasters",
    "href": "raster_data.html#modifying-rasters",
    "title": "\n31  Raster Data\n",
    "section": "\n31.2 Modifying Rasters",
    "text": "31.2 Modifying Rasters\nWe can modify rasters just as easily as we can crop them. They are matrices, after all. The square bracket indexing you use for matrices are just as effective as before.\nIn the next example, I mask the landscape based upon elevation. I create a copy of the original raster and then make everything whose elevation is less than 500m as missing data. Plotting this over the top of the original raster shows only locations where elevation exceeds this cutoff.\n\nbc &lt;- baja_california\nbc[ bc &lt; 500 ] &lt;- NA\nplot( baja_california, legend=FALSE, col=\"darkgrey\" )\nplot( bc, add=TRUE, legend=FALSE)",
    "crumbs": [
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "raster_data.html#extracting-point-data-from-rasters",
    "href": "raster_data.html#extracting-point-data-from-rasters",
    "title": "\n31  Raster Data\n",
    "section": "\n31.3 Extracting Point Data From Rasters",
    "text": "31.3 Extracting Point Data From Rasters\nSo far, rastes have been confined to representing a single static object. However, it is not uncommon to need to query a raster and find out the values at particular points. These points may be pre-defined or they may be dynamic (e.g., you need to point at a location on the map and determine the value there).\nFor queries of the first kind, we can use the extract() function. For this I downloaded the average temperature and precipitation rasters from Worldclim.\n\nbaja_temp &lt;- raster(\"./spatial_data/bio1.tif\")\nbaja_prec &lt;- raster(\"./spatial_data/bio12.tif\")\n\nAnd then extract the values from each of these layers into the coords data we already have set up.\n\ncoords$elevation &lt;- extract( baja_california, coords[,c(2,3)])\ncoords$mean_temp &lt;- extract( baja_temp, coords[,c(2,3)])\ncoords$mean_precip &lt;- extract( baja_prec, coords[,c(2,3)])\ncoords[1:10,]\n\nA note should be made on the temperature and precipitation values. Temperature is denoted in tenths of a degree Celcius. Though it does get quite hot at times, it does not average 188°C at population 88! Similarly, the units for precipitation are mm (or tenths of centimeters if you will…).\n\nlibrary(ggrepel)\ncoords &lt;- coords[ order(coords$Latitude),]\np &lt;- ggplot( coords, aes(x=Latitude,y=elevation)) + geom_line(color=\"lightgrey\") \np &lt;- p + geom_point() + ylab(\"Elevation (m)\")\np + geom_text_repel(aes(label=Stratum), color=\"red\") \n\nThe package ggrepel provides a pseudo-smart labelling geometry for ggplot allowing you to have labels that are shifted around the points so as to maximize visibility.\nFor inquires of the second type, we can use the function click() to retrieve one of several outputs. Here is the help file that describes the various components.\n\nclick {raster} R Documentation\nQuery by clicking on a map\n\nDescription\n\nClick on a map (plot) to get values of a Raster* or Spatial* object at that location; and optionally the coordinates and cell number of the location. For SpatialLines and SpatialPoints you need to click twice (draw a box).\n\nUsage\n\n## S4 method for signature 'Raster'\nclick(x, n=Inf, id=FALSE, xy=FALSE, cell=FALSE, type=\"n\", show=TRUE, ...)\n\n## S4 method for signature 'SpatialGrid'\nclick(x, n=1, id=FALSE, xy=FALSE, cell=FALSE, type=\"n\", ...)\n\n## S4 method for signature 'SpatialPolygons'\nclick(x, n=1, id=FALSE, xy=FALSE, type=\"n\", ...)\n\n## S4 method for signature 'SpatialLines'\nclick(x, ...)\n\n## S4 method for signature 'SpatialPoints'\nclick(x, ...)\nArguments\n\nx   - Raster*, or Spatial* object (or missing)\nn   - number of clicks on the map\nid - Logical. If TRUE, a numeric ID is shown on the map that corresponds to the row number of the output\nxy - Logical. If TRUE, xy coordinates are included in the output\ncell - Logical. If TRUE, cell numbers are included in the output\ntype - One of \"n\", \"p\", \"l\" or \"o\". If \"p\" or \"o\" the points are plotted; if \"l\" or \"o\" they are joined by lines. See ?locator\nshow - logical. Print the values after each click?\n... - additional graphics parameters used if type != \"n\" for plotting the locations. See ?locator\n\nValue\n\nThe value(s) of x at the point(s) clicked on (or touched by the box drawn).\n\nNote\n\nThe plot only provides the coordinates for a spatial query, the values are read from the Raster* or Spatial* object that is passed as an argument. Thus you can extract values from an object that has not been plotted, as long as it spatially overlaps with with the extent of the plot.\n\nUnless the process is terminated prematurely values at at most n positions are determined. The identification process can be terminated by clicking the second mouse button and selecting 'Stop' from the menu, or from the 'Stop' menu on the graphics window.\n\nSee Also\n\nselect, drawExtent\n\nExamples\n\nr \n\n\nHere is the output from a single inquire on the baja_california raster map.\n\nplot( baja_california, legend=FALSE)\nclick(baja_california, xy=TRUE)\n##           x        y value\n## 1 -113.3875 27.82083   116\n\n\n31.3.1 Stacks of Rasters\nIt is not uncommon to be working with many different raster layers at the same time. Instead of loading them individually, the raster library has a RasterStack object that can hold several rasters at one time and can be used in places where we would use individual rasters. Here is an example using the elevation and temperature rasters for tile 22.\n\nfiles &lt;- c(\"./spatial_data/alt.tif\", \"./spatial_data/bio1.tif\", \"./spatial_data/bio5.tif\", \"./spatial_data/bio6.tif\")\nbio_layers &lt;- stack( files )\nbio_layers\n\nclass      : RasterStack \ndimensions : 3600, 3600, 12960000, 4  (nrow, ncol, ncell, nlayers)\nresolution : 0.008333333, 0.008333333  (x, y)\nextent     : -120, -90, 0, 30  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nnames      :  alt, bio1, bio5, bio6 \nmin values : -202,  -20,   54,  -83 \nmax values : 5469,  295,  424,  242 \n\n\nPerforming operations on a stack is as easy as performing them on individual layers. Here, I trim them to the hull defined above.\n\ne &lt;- extent( c(-115,-109,22,30) )\nbio_layers &lt;- crop( bio_layers, e )\nplot(bio_layers)\n\n\n\n\n\n\n\nFrom which values may be extracted using normal methods as outlined above.\n\ndf &lt;- extract( bio_layers, baja_pts)\ndf &lt;- df[ !is.na(df[,1]),]\nhead(df)\n\n     alt bio1 bio5 bio6\n[1,] 681  178  331   48\n[2,] 361  195  346   61\n[3,] 368  197  348   62\n[4,] 240  205  355   68\n[5,] 177  203  352   71\n[6,]  26  223  372   81\n\n\nAnd visualized normally.\n\nlibrary( GGally )\nggpairs( as.data.frame(df))\n\n\n\n\n\n\n\n\n31.4 Rasters & ggplot\nAs is the case with a lot of data types in R, there is a way to use the ggplot library to visualize rasters. Essentially, what you need to do is to transform your raster objects into data.frame objects for ggplot’s geom_tile() function. Here is an example.\n\nlibrary(ggplot2)\ndf &lt;- data.frame( rasterToPoints( baja_california )) \nnames(df) &lt;- c(\"Longitude\",\"Latitude\",\"Elevation\")\np &lt;- ggplot( df ) + geom_tile( aes(x=Longitude,y=Latitude,fill=Elevation)) \np &lt;- p + scale_fill_gradientn( colors=c('#a6611a','#dfc27d','#f5f5f5','#80cdc1','#018571'))\np &lt;- p + coord_equal() + xlab(\"Longitude\") + ylab(\"Latitude\") \np\n\n\n\n\n\n\n\nAs usual, we can add additional information to the plot and as we would for any other ggplot object. Here I’ll add the populations and indicate if they have samples in them that are of one species (Pure) or have a mix of the two (Mixed).\n\nnum.clades &lt;- colSums( table(arapat$Species, arapat$Population) &gt; 0 )\nStratum=names(num.clades)\nSpecies= factor( c(\"Pure\",\"Mixed\")[num.clades] )\ntmp.df &lt;- data.frame( Stratum, Species )\nsites &lt;- merge( coords, tmp.df )\np + geom_point( aes(x=Longitude,y=Latitude, shape=Species), size=3, data=sites )\n\n\n\n\n\n\n\n\n31.5 3D Visualization\nIt is also possible to visualize rasters in 3-space. The library rasterVis provides an interface to the rgl library to plot a surface. Once installed, these are easy to use for viewing surfaces. Here is an example using the elevation data we have been playing with.\n\nlibrary(rasterVis)\nplot3D( baja_california , zfac=0.1)\n\n\nThe zfac option in the plot is the amount to scale the z-axis (elevation) in relation to the x-axis and y-axis dimensions. It is a bit exagerated at zfac=0.1 but you get the idea.\n\n\n\n\n31.6 Saving & Exporting Rasters\nThere are many situations where you need to save a raster you’ve manipulated. As these raster objects are R objects, you can save them directly to the file system using the write() as:\n\nwrite( baja_california, filename=\"baja_california.rda\")\n\nThis will save the raster object to file exactly like it is in your R session. To load it back in you just use load() and it is returned just like it was. The benefit to saving these as R objects is that you do not need to change it at all to pick up where you left off.\nYou may also need to export the raster in a non-R format for external analyses. To do this, you use the writeRaster() function. The file extension is used to determine the file format used and R saves it automatically.\n\nwriteRaster( baja_california, filename=\"baja_california.tif\")",
    "crumbs": [
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "raster_data.html#rasters-ggplot",
    "href": "raster_data.html#rasters-ggplot",
    "title": "\n31  Raster Data\n",
    "section": "\n31.4 Rasters & ggplot",
    "text": "31.4 Rasters & ggplot\nAs is the case with a lot of data types in R, there is a way to use the ggplot library to visualize rasters. Essentially, what you need to do is to transform your raster objects into data.frame objects for ggplot’s geom_tile() function. Here is an example.\n\nlibrary(ggplot2)\ndf &lt;- data.frame( rasterToPoints( baja_california )) \nnames(df) &lt;- c(\"Longitude\",\"Latitude\",\"Elevation\")\np &lt;- ggplot( df ) + geom_tile( aes(x=Longitude,y=Latitude,fill=Elevation)) \np &lt;- p + scale_fill_gradientn( colors=c('#a6611a','#dfc27d','#f5f5f5','#80cdc1','#018571'))\np &lt;- p + coord_equal() + xlab(\"Longitude\") + ylab(\"Latitude\") \np\n\n\n\n\n\n\n\nAs usual, we can add additional information to the plot and as we would for any other ggplot object. Here I’ll add the populations and indicate if they have samples in them that are of one species (Pure) or have a mix of the two (Mixed).\n\nnum.clades &lt;- colSums( table(arapat$Species, arapat$Population) &gt; 0 )\nStratum=names(num.clades)\nSpecies= factor( c(\"Pure\",\"Mixed\")[num.clades] )\ntmp.df &lt;- data.frame( Stratum, Species )\nsites &lt;- merge( coords, tmp.df )\np + geom_point( aes(x=Longitude,y=Latitude, shape=Species), size=3, data=sites )",
    "crumbs": [
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "raster_data.html#d-visualization",
    "href": "raster_data.html#d-visualization",
    "title": "\n31  Raster Data\n",
    "section": "\n31.5 3D Visualization",
    "text": "31.5 3D Visualization\nIt is also possible to visualize rasters in 3-space. The library rasterVis provides an interface to the rgl library to plot a surface. Once installed, these are easy to use for viewing surfaces. Here is an example using the elevation data we have been playing with.\n\nlibrary(rasterVis)\nplot3D( baja_california , zfac=0.1)\n\n\nThe zfac option in the plot is the amount to scale the z-axis (elevation) in relation to the x-axis and y-axis dimensions. It is a bit exagerated at zfac=0.1 but you get the idea.",
    "crumbs": [
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "raster_data.html#saving-exporting-rasters",
    "href": "raster_data.html#saving-exporting-rasters",
    "title": "\n31  Raster Data\n",
    "section": "\n31.6 Saving & Exporting Rasters",
    "text": "31.6 Saving & Exporting Rasters\nThere are many situations where you need to save a raster you’ve manipulated. As these raster objects are R objects, you can save them directly to the file system using the write() as:\n\nwrite( baja_california, filename=\"baja_california.rda\")\n\nThis will save the raster object to file exactly like it is in your R session. To load it back in you just use load() and it is returned just like it was. The benefit to saving these as R objects is that you do not need to change it at all to pick up where you left off.\nYou may also need to export the raster in a non-R format for external analyses. To do this, you use the writeRaster() function. The file extension is used to determine the file format used and R saves it automatically.\n\nwriteRaster( baja_california, filename=\"baja_california.tif\")",
    "crumbs": [
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Raster Data</span>"
    ]
  },
  {
    "objectID": "projections.html",
    "href": "projections.html",
    "title": "\n32  Map Projections\n",
    "section": "",
    "text": "32.1 Projections\nA spatial projection is a mathematical representation of a coordinate space used to identify geospatial objects. Because the earth is both non-flat and non-spheroid, we must use mathematical approaches to describe the shape of the earth in a coordinate space. We do this using an ellipsoid—a simplified model of the shape of the earth. Common ellipsoids include:\nOnto this ellipsoid, we must define a set of reference locations (in 3-space) called datum that help describe the precise shape of the surface.\nA projection onto an ellipsoid is a way of converting the spherical coordinates, such as longitude and latitude, into 2-dimensional coordinates we can use. There are three main types of approaches that have been used to develop various projections. (see wikipedia for some example imagery of different projections).\nThese include:\nAll projections produce bias in area, distance, or shape (some do so in more than one), so there is no ‘optimal’ projection. To give you an idea of the consequences of these projections, I’ll use the United States map as an example and we can visualize how it is projected onto a 2-dimensional space using different projections.",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Map Projections</span>"
    ]
  },
  {
    "objectID": "projections.html#projections",
    "href": "projections.html#projections",
    "title": "\n32  Map Projections\n",
    "section": "",
    "text": "Azimuthal: An approach in which each region of the earth is projected onto a plane tangential to the surface, typically at the pole or equator. Cylindrical: This approach projects the surface of the earth onto a cylinder, which is ‘unrolled’ like a large map. This approach ‘stretches’ distances in a east-west fashion, which is why Greenland looks so large…\nConic: Another ‘unrolling’ approach, though this time instead of a cylinder, it is projected onto a cone.\n\n\n\n32.1.1 Equatorial Projections\nThese are projections centered on the Prime Meridian (Longitude=0)\nMercator Projection\n\nlibrary(maps)\nmap(\"state\",proj=\"mercator\")\n\n\n\n\n\n\n\nMollWeide Projection\n\nmap(\"state\",proj=\"mollweide\")\n\n\n\n\n\n\n\nGilbert Projection\n\nmap(\"state\",proj=\"gilbert\")\n\n\n\n\n\n\n\nCylequalarea Projection\nSome projections require additional parameters, this one is based upon equally spaced and straight meridians, equal area, and true centered on a particular Latitude. I used the centroid of the US.\n\nmap(\"state\",proj=\"cylequalarea\",par=39.83)\n\n\n\n\n\n\n\n\n32.1.2 Azimuth Projections\nThese projections are centered on the North Pole with parallels making concentric circles. Meridians are equally spaced radial lines.\nOrthographic Projection\n\nmap(\"state\",proj=\"orthographic\")\n\n\n\n\n\n\n\nStereographic Projection\n\nmap(\"state\",proj=\"orthographic\")\n\n\n\n\n\n\n\nPerspective Projection\nHere the parameter is the distance (in earth radii) the observer is looking.\n\nmap(\"state\",proj=\"perspective\", param=8)\n\n\n\n\n\n\n\nGnomonic Projection\n\nmap(\"state\",proj=\"gnomonic\")\n\n\n\n\n\n\n\n\n32.1.3 Polar Conic Projections\nHere projections are symmetric around the Prime Meridian with parallel as segments of concentric circles with meridians being equally spaced.\n\nmap(\"state\",proj=\"conic\",par=39.83)\n\n\n\n\n\n\n\n\nmap(\"state\",proj=\"lambert\",par=c(30,40))\n\n\n\n\n\n\n\n\n32.1.4 Miscellaneous Projections\nSquare Projection\n\nmap(\"state\",proj=\"square\")\n\n\n\n\n\n\n\nHexagon Projection\n\nmap(\"state\",proj=\"hex\")\n\n\n\n\n\n\n\nBicentric Projection\n\nmap(\"state\",proj=\"bicentric\", par=-98)\n\n\n\n\n\n\n\nGuyou Projection\n\nmap(\"state\",proj=\"guyou\")\n\n\n\n\n\n\n\nThere are a lot of ways to project a 3-dimensional surface onto a 2-dimensional representation. Be aware of what you are using and how you are using it when plotting materials.\n\n32.1.5 Reprojecting Rasters\nWhen working with rasters, we can reproject these onto other projections rather easily. Here is an example from the worldclim elevation tile we used previously (see @ref(cropping-rasters)).\n\nlibrary(raster)\nalt &lt;- raster(\"./spatial_data/alt.tif\")\ne &lt;- extent( c(-115,-109,22,30) )\nbaja_california &lt;- crop( alt, e )\nbaja_california\n\nclass      : RasterLayer \ndimensions : 960, 720, 691200  (nrow, ncol, ncell)\nresolution : 0.008333333, 0.008333333  (x, y)\nextent     : -115, -109, 22, 30  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : alt \nvalues     : -202, 2263  (min, max)\n\n\nWe can now project it to another projection, lets say Lambert Conic Conformal.\n\nlibrary(rgdal)\nprojection(baja_california)\nbaja_lcc &lt;- projectRaster( baja_california, crs=\"+proj=lcc +lat_1=48 +lat_2=33 +lon_0=-100 +ellps=WGS84\")\nbaja_lcc\n\nThese two projections influence the region as shown below.\n\npar(mfrow=c(1,2))\nplot(baja_california, legend=FALSE, xlab=\"Longitude\",ylab=\"Latitude\")\nplot(baja_lcc, legend=FALSE )\npar(mfrow=c(1,1))\n\n\n32.1.6 Projecting In GGPlot\nAs usual, there is probably a way to plot these values in ggplot to make the output just a little bit more awesome. Projections of data in ggplot displays can be manipulated by appending a coord_* object to the plot. Here are two examples using a mercator and azimuth equal area projection of the state maps.\n\nlibrary(ggplot2)\nstates &lt;- map_data(\"state\")\nmap &lt;- ggplot( states, aes(x=long,y=lat,group=group))\nmap &lt;- map + geom_polygon( fill=\"white\",color=\"black\")\nmap &lt;- map + xlab(\"Longitude\") + ylab(\"Latitude\")\nmap + coord_map(\"mercator\") \n\n\n\n\n\n\n\nConversely, we can plot it using the equal area Azimuth projection\n\nmap + coord_map(\"azequalarea\")\n\n\n\n\n\n\n\nor fisheye\n\nmap + coord_map(\"fisheye\",par=3)\n\n\n\n\n\n\n\nor any other projection available listed in the mapproject() function.",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Map Projections</span>"
    ]
  },
  {
    "objectID": "projections.html#coordinate-systems",
    "href": "projections.html#coordinate-systems",
    "title": "\n32  Map Projections\n",
    "section": "\n32.2 Coordinate Systems",
    "text": "32.2 Coordinate Systems\nIn R, we typically are dealing with a combination of data that we’ve collected and that we’ve attained from some other provider. In most GIS applications, the coordinate systems we encounter are either:\n\nUTM (Universal Transverse Mercator) measuring the distance from the prime meridian for the x-coordinate and the distance from the equator (often called northing in the northern hemisphere) for the y-coordinate. These distances are in meters and the globe is divided into 60 zones, each of which is 6 degrees in width. Geographic coordinate systems use longitude and latitude. For historical purposes these are unfortunately reported in degrees, minutes, seconds, a temporal abstraction that is both annoying and a waste of time (IMHO).\n\nDecimal degrees, while less easy to remember, are easier to work with in R.\nState Planar coordinate systems are coordinate systems that each US State has defined for their own purposes. They are based upon some arbitrarily defined points of reference and another pain to use (IMHO). Given the differential in state area, some states are also divided into different zones. Maps you get from municipal agencies may be in this coordinate system. If your study straddles different zones or even state lines, you have some work ahead of you…\n\nIt is best to use a system that is designed for your kind of work. Do not, for example, use a state plane system outside of that state as you have bias associated with the distance away from the origin. That said, Longitude/Latitude (decimal degrees) and UTM systems are probably the easiest to work with in R.\nIn R, we use rgdal to project points. Here I load in the coordinates of the populations in the Arapatus attenuatus data set and make a SpatialPoints object out of it. Setting the proj4string() here does not project the data, I am just specifying that the data are already in the lat/long WGS84 format.\n\nlibrary(sp)\nlibrary(gstudio)\ndata(arapat)\ncoords &lt;- strata_coordinates( arapat )\npts &lt;- SpatialPoints( coords[,2:3] )\nproj4string(pts) &lt;- CRS(\"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\")\npts\n\nclass       : SpatialPoints \nfeatures    : 39 \nextent      : -114.2935, -109.1263, 23.0757, 29.32541  (xmin, xmax, ymin, ymax)\ncrs         : +proj=longlat +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +no_defs \n\n\nThe CRS() function holds the definition of the projection and interfaces between the PROJ.4 and RGDAL libraries. To project a set of data points into a new coordinate systems, we use spTransform() and pass it the definition of the new system to use.\n\n32.2.1 Changing Datum\nWe will first begin by looking at differences in the actual datum used to record the loation of plots. Here I compare the decimal Longitude/Latitude we’ve used thus far with that from the Universal Transverse Mercator (UTM).\n\npts.utm &lt;- spTransform(pts, CRS(\"+proj=utm +zone=12 +datum=WGS84\"))\nsummary( pts.utm )\n\nObject of class SpatialPointsDataFrame\nCoordinates:\n              min       max\ncoords.x1  180128  686925.2\ncoords.x2 2552540 3248545.0\nIs projected: TRUE \nproj4string :\n[+proj=utm +zone=12 +datum=WGS84 +units=m +no_defs]\nNumber of points: 39\n\n\nYou can see the tranformations in the coordinate system by comparing the plots below. The relative position of each point is the same.\n\npar(mfcol=c(1,2))\nplot( pts, axes=TRUE, xlab=\"Longitude\", ylab=\"Latitude\" )\nplot( pts.utm, axes=TRUE, xlab=\"Easting\",ylab=\"Northing\" )\n\n\n\n\n\n\npar(mfcol=c(1,1))",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Map Projections</span>"
    ]
  },
  {
    "objectID": "ecological_distance.html",
    "href": "ecological_distance.html",
    "title": "\n33  Ecological Distance\n",
    "section": "",
    "text": "33.1 The Concept of Resistance\nPreviously we have been focused on population separation in Euclidean space (e.g., physical distance). However, populations may be functionally separated by features of the intervening landscape. It is entirely possible that features existing between populations may influence the ability of gene flow to maintain connectivity between sites.\nIt can be argued that the sine qua non of applied population genetics (or landscape genetics if you like that term) is in the identification of the which subset of ecological or other features influence the connectivity of populations. Throughout the history of population genetics, a lot of theory has been developed ignoring the effects of other organisms, the environment in which micro evolutionary processes are occurring, etc. In the last few decades, the field has moved towards more inclusive, and site-aware, approaches and in this chapter we get into some of the ways in which you can include data available to you in both Vector (@ref(vector-data)) and Raster (@ref(raster-data)) formats.\nFundamental to the newer ways we are analyzing our data is the concept of resistance. While there may be many kinds of resistance, they are can all be described by a simple fundamental notion that there exists a quantitative value that can be assigned to the separation of locales on a landscape. Consider the inset images in Figure @ref(fig-legos), stylized using a Lego representation of myself and a tasty meal.\nThese different external resistance examples represent:\nIn all three of these cases, the underlying distance matrix (or raster in our representation) is estimated and filled with values. For IBD, it is uniform in magnitude across the matrix. Barriers have one or more spatially contiguous features with increased, or perhaps reduced if we consider them as corridors—the math is exactly the same. For the more complicated scenario, the resistance matrix has a wide range of resistance values associated with indiviudal cells. Despite the aparent differences, the underlying approahces are identical, we define a resistance matrix and then use some measure of separation to measure how far apart each of the points of interest are located. The important point is that the fundamental concept of resistance underlies all these different ‘models’. In fact, you could argue (and you would be statistically correct in doing so) that even the Island Model and Stepping Stone models are resistance models.\nAs a demonstration of how we do the actual calculations in R, we will start with the venerable arapat data set. Only a subset of populations will be used for the following reasons:\nTo start with, I will use the elevation raster and modifications thereof. The ultimate goal here is to estimate ecological separation using a variety of algorithms and as such we need to minimize the size of the background raster as much as possible. In the code below, I take the elevation raster and create the convex hull around the population locations with a smallish buffer around the points so that the outermost populations will not be on the very edge of the data. I further constrain this raster by cutting regions within which we have no evidence that the target species exist, in this case we have almost no records of it occurring above 700m.\nTo start with, we should probably examine the distribution of the variable we are working with in the locations at which the samples are taken.\nWe can compare this against the ‘background’ values that we find across the raster, even in places where our samples were not taken. Deviations observed in the distribution of values sampled from our sites from that defined in the background suggest that our sampling was not entirely random see @ref(niche-modeling) for a more complete discussion on how to evaluate the relative importance of particular background features in the distribution of your organism.\nThe way in which the intervening habitat may influence genetic connectivity can be both varied and complex. In R we quantify spatial data using a raster format. As discussed in @ref(raster-data), a raster is essentially a matrix whose entries represent some value that has both spatial location and extent.\nSeveral approaches are available to parameterizing a cost surface. These range from expert opinion (where someones opinion fills in the gaps), to externally defined variables, to approaches that consider entire ranges of resistance.\nknitr::include_graphics(\"./media/RasterGridParameterized.png\")\n\n\n\nDichotomous cost surface containing two features, one four times more difficult to traverse than the other.\nIndependent of how the values are set (though this is a huge component which can be addressed outside the context of how this is done), the raster (as in Figure @ref(fig:parameterizedRaster)) can be considered as a component of a potential hypothesis. It is not a complete hypothesis because we must first decide how distances on that raster are estimated—the result of which will be the complete hypothesis. We return to ways in which distances can be estimated below (@ref(estimating-separation)).\nknitr::include_graphics(\"./media/MultivariateCostRaster.png\")\n\n\n\nIndividual resistance surfaces may be combined to produce interaction (multivariate) cost distance surfaces. In this example, the categorial surface (left) is combined with a cost sufrace representing a cline (center raster) to produce via the addition operator a surface representing both (right).\nOnce we have a set of cost surfaces defined, our goal is to determine if there are subsets which help to explain the observed genetic structure we find on the landscape (Figure @ref(fig:EuclVsEcol)).\nknitr::include_graphics(\"./media/EuclideanVsEcological.png\")\n\n\n\nTwo different cost surfaces, Euclidean distance (left) and the multivariate ecological distance derived perviously (right). The extent to which each of these resistance hypotheses can explain the observed genetic distances (graphs over top\nNo matter how we actually measure separation, we can think of cost surfaces as belonging to one of two mutually exclusive categories; absolute resistance and relative resistance. Absolute resistance is invariant to source and destination whereas relative landscape resistance is depending upon starting location.",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Ecological Distance</span>"
    ]
  },
  {
    "objectID": "ecological_distance.html#the-concept-of-resistance",
    "href": "ecological_distance.html#the-concept-of-resistance",
    "title": "\n33  Ecological Distance\n",
    "section": "",
    "text": "33.1.1 Absolute Resistance\nAbsolute resistance is the condition by which landscape features provide an invariable source of resistance to movement. Examples here may include physical barriers that exert the same magnitude of resistance to movement no matter where the initial and destination locations are found. For example, in an urban environment, a building provides an absolute resistance with respect to plant pollinators—they cannot move through that object no matter which direction they are coming from.\nIn the example below, I will use the slope of the terrain as a example under the assumption that there a level of steepness of the terrain that prevents movement of individuals between locales. It does not matter at which direction the organism approaches this steep part of the landscape (uphill, downhill, or cross slope), their movement is restricted in a uniform manner.\n\nslope &lt;- terrain(alt, unit='degrees')\nplot(slope)\nplot(baja_pts,add=T)\n\nIf we look at the distribution of slope values we can see the vast majority of the lower part of Baja California has a rather gentle slope.\n\ndf &lt;- data.frame( rasterToPoints(slope))\nggplot( df, aes(x=slope)) + geom_histogram( binwidth=0.1) + \n  xlab(\"Slope (degrees)\") + ylab(\"Frequency\")\n\nIn the following examples, I’m going to use the distribution of values shown in Figure @ref(fig:distribution-of-slope).\n\n\n\nIt is very important to note that the slope we are deriving here is based upon an average across a 30-arc second pixel size in the desert. We are perhaps missing a lot of sub-kilometer slop-y areas that prevent movement. However, this is an example meant more for the importance of providing the methodology of how you would do this than an expectation explaining biological relevance.\n\n\n\nFor the purposes of this example, I’ll define three categories of slope. The values returned by the terrain() function in this case are in degrees (though radians and tangent are also possible, see ?terrain). A new raster will be created, based slope, with the following (somewhat arbitrary but relevant as an example) categories:\n\nSlope &lt; 5\\(^{\\circ}\\) has no effect on movement.\nSlope at a location between 5\\(^{\\circ}\\) and 10\\(^{\\circ}\\) result in a movement resistance twice as difficult as the low/no slope condition.\nSlope in excess of 10\\(^{\\circ}\\) is categorized as four times as difficult for movement than the low/no slope areas on the map.\n\n\ntmp &lt;- slope\ntmp[ slope &lt; 5 ] &lt;- 1\ntmp[ slope &gt; 5 ] &lt;- 2\ntmp[ slope &gt; 10 ] &lt;- 4\ncat_slope &lt;- ratify(tmp, count=TRUE)\nrat &lt;- levels( cat_slope )[[1]]\nrat$Slope_Type &lt;- c(\"None\",\"Low\",\"High\")\nlevels(cat_slope) &lt;- rat \ncat_slope\n\nFor this raster, I set up the categories and define a raster attribute table (see @ref(categorical-rasters)) to make the output more legible. If we visualize this raster, we see the landscape partitioned into these three categories.\n\nplot(cat_slope, legend=FALSE)\n\nFrom here, we can proceed to estimating distances among points. This absolute cost surface is just one example. You could have used the raw value of slope (larger linearly becoming more difficult), a function of slope, or any variant of coming up with a metric to create the values in each pixel. The point here is that this is an absolute cost, and all estimates of distance, independent of origin and destination, use the same cost raster.",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Ecological Distance</span>"
    ]
  },
  {
    "objectID": "ecological_distance.html#relative-resistance",
    "href": "ecological_distance.html#relative-resistance",
    "title": "\n33  Ecological Distance\n",
    "section": "\n33.2 Relative Resistance",
    "text": "33.2 Relative Resistance\nThe other way in which rasters may be created are are based upon relative cost distances. These cost surfaces are created, relative to some particular benchmark point on the surface. For example, if you think that phenology (e.g., the timing of reproduction) may be an important component contributing to connectivity among populations (as is the case for plant species), it is not the absolute value of elevation that is of concern across the landscape. Rather it is the similarity in elevation that is the driver—plant populations at the same elevations may tend to flower in higher synchrony than those separated across elevation gradients. As such, when we estimate cost surfaces for something like this, we would focus on on the magnitude of elevation but on the absolute difference in elevation between points. Under this relative cost approach, the distance from each locale would be based upon the deviance in elevation from that point to the rest of the landscape.\nI’ll use this concept of elevation similarity in this example, starting with the elevation of population 163, which will be the benchmark.\n\nidx &lt;- which( coords$Stratum == \"163\")\ntarget_elevation &lt;- coords$elevation[ idx ]\ntarget_elevation\n\nFrom this target elevation, we can create an elevation raster that is based upon the deviation between the benchmark and everyone else. Since this is a cost surface we are creating we need to perhaps include a base cost instead of having elevation clines with zero resistance. For this, I add a unit cost to the entire raster.\n\ntmp &lt;- alt - target_elevation\ntmp &lt;- abs( tmp ) + 1\nplot( tmp )\n\nFor subsequent analyse, the distance between this specific locale and the remaining can be estimated using this raster. However, if we are to move to the next population, the distance between that locale and this one would need to be based on a new elevation benchmark. This means that for \\(K\\) populations, pairwise estimates of relative distance need to be estimated from \\(K\\) different rasters1!",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Ecological Distance</span>"
    ]
  },
  {
    "objectID": "ecological_distance.html#estimating-separation",
    "href": "ecological_distance.html#estimating-separation",
    "title": "\n33  Ecological Distance\n",
    "section": "\n33.3 Estimating Separation",
    "text": "33.3 Estimating Separation\nOnce we have defined one or more rasters quantifying resistance costs, we can go and estimate the distance among points on those rasters. There are several ways available for us to estimate distance. The differences in these methods are based upon the algorithm used to estimate separation and the extent of the raster used to apply these algorithms.\n\n33.3.1 Least Cost Distance\nA least cost path is the route between two points, say \\(A \\to B\\), whose measured cost is minimal. This is the optimal route across the landscape. Organisms or vectors of gene exchange may, or may not, move across a landscape following an optimal path. An example shortest path is shown in Figure @ref(fig:lcp-example).\n\nknitr::include_graphics(\"./media/LCP.png\")\n\n\n\nEstimation algorithm for least-cost path distance. The path chosen is the one with the shortest overall distance, or at least one of the paths with equally short overall distances.\n\n\n\nOne way to estimate distances in R, is through the use of the gdistance library. In this approach, we define a transition object based upon:\n1. The cost distance raster. By default, the transition() function works on conductance, which is the inverse of resistance. In these examples, we have used a single raster, though use of RasterBrick objects is just as appropriate. 2. The function by which we estimate the pixel-to-pixel distances.\n3. The neighborhood around each pixel that we look at during each step. Options for this include:\n- A von Neumann neighborhood (directions=4) consists of the pixels immediately above, below, and on both sides of the target pixel.\n- A Moore’s neighborhood (directions=8) consisting of all pixels surrounding the target pixel (e.g., even the diagonal ones) - A Knight & One-Cell Queen move (directions=16), which adds the next layer of cells outside of Moore’s neighborhood.\nOnce estimated, the transition object must be corrected for if you are either using a large extent based upon Latitude and Longitude datum (e.g., they are not unit-wise consistent in area), or you have used a direction option other than the von Neuman Neighborhood.\nIn the example below, I use the cost distance estimated based upon similarity in elevation, under the hypothesis that phenological synchrony may influence connectivity.\n\nlibrary(gdistance)\ntr &lt;- transition( 1/tmp, transitionFunction = mean, directions = 4 )\ntr &lt;- geoCorrection( tr, type=\"c\", multpl=FALSE, scl=FALSE)\ntr\n\nFrom this, potentially corrected, transition object we can estimate shortest path distance using the aptly named function shortestPath(), passing it the transition object, the point from which we are starting, and the destination, and an optional parameter on how we want the output returned. In the example below, the results is returned as a SpatialLines object which I use to plot.\n\npath.1 &lt;- shortestPath( tr, baja_pts[26], baja_pts[15], output=\"SpatialLines\")\nplot( tmp , xlab=\"Longitude\", ylab=\"Latitude\")\nlines( path.1, col=\"red\")\npoints( coords[c(26,15),2:3],pch=16, col=\"red\")\n\nAs explained above, relative cost distances may create asymmetric distances. If the shortest path between the two populations is taken in the opposite direction, we get different shortest path across the landscape.\n\ntmp &lt;- alt\ntmp &lt;- abs( tmp - coords$elevation[ 15 ] ) + 1\ntr &lt;- transition( 1/tmp, transitionFunction = mean, directions = 4 )\ntr &lt;- geoCorrection( tr, type=\"r\", multpl=FALSE, scl=FALSE)\npath.2 &lt;- shortestPath(tr,baja_pts[15],baja_pts[26],output=\"SpatialLines\")\nplot( alt , xlab=\"Longitude\", ylab=\"Latitude\")\nlines( path.1, col=\"red\")\nlines( path.2, col=\"blue\")\npoints( coords[c(26,15),2:3],pch=16, col=\"red\")\n\nNot only is it a different path, they are also different lengths.\n\nc( SpatialLinesLengths( path.1 ), SpatialLinesLengths( path.2 ) )\n\n\n33.3.2 All Paths (Circuit) Distance\nOrganisms may not move in an optimal way across the landscape but may move along several different paths, most of which are unoptimal. The relative proportion of these alternative paths and their lengths may be combined to produce a distance between sites based upon all potential paths, \\(A \\Rightarrow B\\). In the context of genetic connectivity, this method is referred to by the name of one of the software packages available to estimate it, Circuitscape by McRae (2006). An example is shown in Figure @ref(fig:ct-example).\n\nknitr::include_graphics(\"./media/CT.png\")\n\n\n\nEstimation algorithm for all-paths (circuit) distance. The estimated distance between points \\(A\\) and \\(B\\) is based upon the length and relative frequencies of all potential paths across the landscape, even thouse that traverse more resistant portions of the landscape (highlighted in yellow).\n\n\n\nUsing as an example, you can apply this algorithm to the resistance surface based upon relative deviation from population 93 (as in the last example). Since this method uses all paths that leave the the target site and can arrive at the destination location, you cannot produce a single path example. What you do get from it is a spatial current raster from that site (e.g., the spatial flow away from it) as shown in Figure @ref(fig:circuitscape-from-93).\n\nr &lt;- raster(\"media/Baja_Out_curmap_15.asc\")\nplot( log10(r), legend=FALSE, xlab=\"Longitude\", ylab=\"Latitude\" )\nplot( baja_pts[15], add=TRUE )\n\n\n33.3.3 Path & Corridor Constraints\nBoth least-cost, and all-paths approaches consider the entire cost surface in the estimation of distances. This is a very important point that needs to be made to emphasize some of the constraints that may be imposed on your actual estimation process.\nThese approaches use graph-theoretic algorithms to estimate the length of various paths across the cost surface. To do this, each pixel is treated as a node in a network, only after which the length of the paths may be estimated. This is why it is important to limit the potential size of your background cost raster to an area that includes your sampling sites but not regions through which connectivity will not traverse. For example, there is a very large number of paths that exist outside the convex hull displayed in Figure @ref(fig:eco-convex-hull), the overwhelming majority of which have absolutely no influence on connectivity among the sites we are interested in studying. If you do not restrict the cost raster as we did, the extra pixels are added network that needs to be constructed to estimate distance, independent of the fact that they may not be a member of the shortest (or even not longest) paths contributing to connectivity. Numerically, algorithms such as Dijkstra’a (one for finding the single shortest path), scale in time with the number of nodes as show below (Figure @ref(fig:dijkstra-time)).\n\nminNodes &lt;- 2\nmaxNodes &lt;- 100\nstepNodes &lt;- 2\nv &lt;- seq(minNodes,maxNodes,by=stepNodes)\ne &lt;- v* (v-1)/2\ndf &lt;- data.frame( Pixels=v, Time=e+v*log10(v))\nggplot(df, aes(Pixels,Time)) + geom_line() + geom_point() + xlab(\"Number of Pixels in Raster\") + ylab(\"Time Complexity\")\n\nComputer scientists use the notion of time complexity, a measure defined as the number of steps in the code required to complete the task. This algorithm is described as \\(O(|e| + |v| \\log |v|\\), where \\(|e|\\) is the number of edges and \\(|v|\\) is the number of nodes. In our example, the number of nodes, \\(|v|\\) is how many pixels are in the landscape and the number of edges is \\(|v|(|v|-1)/2\\)! This gets big quickly so anything you can do to limit the number of potential pixels in your landscape is important.\nIn addition to limiting the size of the raster outside your study area, one can also limit the intervening areas by imposing some constraints on where these paths may be found. A common approach here is to define corridors between sites. These corridors have a restricted width through which these paths may traverse (or at least through which the algorithms may look for these paths). The width of these corridors may be either defined a priori or determined as a component of the analysis itself.",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Ecological Distance</span>"
    ]
  },
  {
    "objectID": "ecological_distance.html#estimating-correlations-in-distance",
    "href": "ecological_distance.html#estimating-correlations-in-distance",
    "title": "\n33  Ecological Distance\n",
    "section": "\n33.4 Estimating Correlations in Distance",
    "text": "33.4 Estimating Correlations in Distance\nIn the end, we are interested in getting to the pairwise distances among all sites, we extend the paths approaches as we outlined above. For these examples, I’ll use the categorical slope example and use both the least cost and the all-paths approaches. Here is the raster.\n\nplot(cat_slope, legend=FALSE, xlab=\"Longitude\",ylab=\"Latitude\", \n     main=\"Categorical Slope Resistance\")\nplot( baja_pts, add=TRUE )\n\n\n33.4.1 Pairwise Least Cost Path\nFor the least cost approach, we need to grab the transition objects and perform a geoCorrection because we are using Longitude & Latitude based coordinates.\n\ntr &lt;- transition( 1/cat_slope, transitionFunction = mean, directions = 4 )\ntr &lt;- geoCorrection( tr, type=\"c\", multpl=FALSE, scl=FALSE)\n\nThe corrected transition object can be used to estimate the shortest paths among all points using the costDistance() function.\n\neDist &lt;- costDistance( tr, baja_pts )\neDist &lt;- as.matrix( eDist )\nrownames(eDist) &lt;- colnames(eDist) &lt;- coords$Stratum\neDist[1:10,1:10] \n\nThis returns a dist object, which I translate into a matrix and you can plot the corresponding relationship with genetic distance (I’ll use Nei’s distance as the example).\n\ngDist &lt;- genetic_distance(data,stratum=\"Population\", mode=\"nei\")\ndf &lt;- data.frame( Genetic_Distance = gDist[lower.tri(gDist)],\n                  Slope_Distance = eDist[ lower.tri(eDist)])\ndf &lt;- df[ !is.infinite(df$Slope_Distance),]\nggplot(df,aes(x=Slope_Distance,y=Genetic_Distance)) + geom_point() + \n  stat_smooth(method=lm, formula = y ~ x) + xlab(\"Least Cost Distance (Slope Categories)\") + ylab(\"Neis Distance\")\n\nFrom these data, we can estimate the correlation (a rough estimate).\n\ncor(df$Genetic_Distance, df$Slope_Distance)\n\nAs we see, it is not a very large correlation. If we were to attempt to ascertain significance for these values, we would want to use a mantel() approach rather than one using a normal Pearson approximation for significance due to the lack of independence. However, there is no real need to test significance here since the correlation is so small, even if this was significantly different than zero the biological consequences are minimal.\n\n33.4.2 Pairwise All Pairs\nFor an all-pairs (circuit) approach, you can do it in R or use the Circuitscape software. I’ll use the latter approach and illustrate the process so that I can demonstrate how you could produce the required input file formats. The input required for Circuitscape consist of a resistance raster and the coordinates of all the sites. For the cost raster, you can write it using writeRaster(). An easy input format for both this raster and the coordinates is that of an ASCII raster. Here is how the cost distance is exported to files.\n\nwriteRaster( cat_slope, filename=\"CategoricalSlope.asc\")\n\nAnd the sites can be done as well in ASCII raster format as by using the rasterize() function turning a set of points into a raster with the same extent as the raster you passed.\n\nsites &lt;- resterize(baja_pts, cat_slope )\nwriteRaster(sites,filename=\"sites.asc\")\n\nThese are input directly into the Circuitscape input. The output from the analysis is presented as a set of columns in a text file (row, col, resistance). Using the same input data as above produced the following:\n\ndata &lt;- read.table(\"media/BajaParwise_resistances_3columns\",header=FALSE, sep=\" \")\nctDist &lt;- matrix(0,nrow=34,ncol=34)\nfor( i in 1:nrow(data)){\n  ctDist[ data$V1[i], data$V2[i] ] &lt;- data$V3[i]\n}\nctDist &lt;- ctDist + t(ctDist)\nctDist[1:5,1:5]\n\n     [,1]       [,2]      [,3]      [,4]       [,5]\n[1,]    0 -1.0000000 -1.000000 -1.000000 -1.0000000\n[2,]   -1  0.0000000  1.142579  3.709424  0.9045455\n[3,]   -1  1.1425792  0.000000  3.708067  1.4392680\n[4,]   -1  3.7094243  3.708067  0.000000  3.6733412\n[5,]   -1  0.9045455  1.439268  3.673341  0.0000000\n\n\nPairs of points that are unreachable are encoded as -1, I’ll change that to NA.\n\nctDist[ ctDist == -1 ] &lt;- NA\nctDist[ is.infinite(ctDist)] &lt;- NA\nctDist[1:5,1:5]\n\n     [,1]      [,2]     [,3]     [,4]      [,5]\n[1,]    0        NA       NA       NA        NA\n[2,]   NA 0.0000000 1.142579 3.709424 0.9045455\n[3,]   NA 1.1425792 0.000000 3.708067 1.4392680\n[4,]   NA 3.7094243 3.708067 0.000000 3.6733412\n[5,]   NA 0.9045455 1.439268 3.673341 0.0000000\n\n\nAs before, we can plot these against each other.\n\ndf1 &lt;- data.frame( Genetic_Distance = gDist[ lower.tri(gDist) ],\n                  Circuit_Distance = ctDist[ lower.tri(ctDist)])\ndf1 &lt;- df1[ !is.na(df1$Circuit_Distance),]\nggplot(df1,aes(x=Circuit_Distance,y=Genetic_Distance)) + geom_point() + \n  stat_smooth(method=lm, formula = y ~ x) + xlab(\"Circuit Distance (Slope)\") + ylab(\"Nei's Genetic Distance\")\n\nIf you do these analyses in R using gdistance, you need to adjust the arguments to both transition and geoCorrection for using an all-paths approach (see the documentation on these functions) and you can get pairwise distances from the commuteDistance() function.\n\ncor( df1$Genetic_Distance, df1$Circuit_Distance )\n\n\n33.4.2.1 Similarity in Outcome\nA pertinent question revolves around the extent to which\n\ndata &lt;- data.frame(Slope=df$Slope_Distance, Circuit=df1$Circuit_Distance )\nggplot(data,aes(x=Slope,y=Circuit)) + geom_point() + \n  stat_smooth() + xlab(\"Least Cost Distance\") + ylab(\"Circuit Distance\")\n\nAs a first pass, we can look at the magnitude of these distance estimates.\n\ncor.test(data$Slope, data$Circuit, method = \"spearman\")",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Ecological Distance</span>"
    ]
  },
  {
    "objectID": "ecological_distance.html#footnotes",
    "href": "ecological_distance.html#footnotes",
    "title": "\n33  Ecological Distance\n",
    "section": "",
    "text": "Assuming of course that each of your \\(K\\) locale are at a unique elevation, if some are at the same elevation then the derived rasters will be identical.↩︎",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Ecological Distance</span>"
    ]
  },
  {
    "objectID": "niche_modeling.html",
    "href": "niche_modeling.html",
    "title": "34  Niche Modeling",
    "section": "",
    "text": "install.packages(c(\"rJava\",\"dismo\"))\nlibrary(\"rJava\")\nlibrary(\"dismo\")\n\nSome of the more common approaches used to characterize a niche is through the use of either logistic regression approaches or maximum entropy. At present, the implementation of the latter appoach, dentoed as MaxEnt, is widely used and will be highlighted in this section. For you to use this, you need to Download the java interface for MaxEnt. You will have to sign in to download the files. Use the .zip archive for this, it has all the files you need. This is not the GUI interface, it is only the underlying java machinery the GUI uses. On some platforms, you may also need to download the legacy Java runtime (as of the writing of this it was SE 6) for your computer to run the analysis. The underlying analyses are done in Java and only accessable to R using this approach.\nUnzip those files and put each of them in the directory defined by the following command:\n\nsystem.file(\"java\", package=\"dismo\")\n\n[1] \"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/dismo/java\"\n\n\nThis puts the java executables into a directory within dismo. This allows the dismo functions to use the latest version of maxent directly. To get started, allow the java executables to access up to 1Gig of your RAM by setting the following option prior to loading in dismo\n\noptions(java.parameters = \"-Xmx1g\" )\nlibrary(dismo)\n\nNow we can get to work. In this example, I’ll use biolayers found to be informative for the distribution of Euphorbia lomelii (Euphrobiaceae), the host plant for Arapatus attenuatus that we collected from both observations and all available herbarium specimen records that had sufficient spatial specificity.\nI load these in using the raster() function. These rasters have already been cropped to the extent enclosing all the arapat populations (as in @ref(cropping-rasters)). I have these stored in a subdirectory called ‘spatial_data’ and load them in as a RasterStack object (a set of rasters).\n\nlibrary(raster)\nfiles &lt;- c( \"bio2.tif\", \"bio7.tif\", \"bio8.tif\", \"bio13.tif\", \n            \"bio15.tif\", \"bio17.tif\")\nfiles &lt;- paste(\"./spatial_data\",files,sep=\"/\")\nbio_layers &lt;- stack(files)\n\nNext, I load in the spatial coordinates of the recorded sites of known occurrence. I set aside 20% of the observed sites to use as a test of the model we derive.\n\nsites &lt;- read.csv(\"./spatial_data/EuphorbiaAllLocations.csv\",header=TRUE)\nnum_train &lt;- round(nrow(sites)*.8)\nidx &lt;- sample( 1:nrow(sites), size=nrow(sites), replace=FALSE )\npts.train &lt;- sites[ idx[1:num_train] , 2:3]\npts.test &lt;- sites[ idx[ (num_train+1):length(idx)], 2:3]\nc( Train=nrow(pts.train), Test=nrow(pts.test) )\n\nTrain  Test \n  301    75 \n\n\nThe maxent model itself is used determine the extent to which site-specific factors may be able to predict the presence of this species.\n\nfit &lt;- maxent(bio_layers,pts.train)\n\nThe html output from the analysis can be viewed in your default browser by showing the model.\n\nfit\n\nWhich produces output that looks something like the following inset.\n\n\nHere are some quick ways to view and interpret output from the maxent approach.\n\nplot(fit)\n\n\nresponse(fit)\n\n\nr &lt;- predict( fit, bio_layers )\nplot(r, xlab=\"Longitude\",ylab=\"Latitude\")\npoints( pts.train, pch=3, cex=0.75)\npoints( pts.train, pch=16, cex=0.75)\n\n\npts.random &lt;- randomPoints(bio_layers, 1000)\nfit.eval &lt;- evaluate(fit, p=pts.test, a=pts.random, x=bio_layers)\nfit.eval\n\n\nvals.pred &lt;- data.frame( extract( bio_layers, pts.test) )\nvals.rand &lt;- data.frame( extract( bio_layers, pts.random) )\nfit.eval_rnd &lt;- evaluate(fit, p=vals.pred, a=vals.rand)\nfit.eval_rnd\n\n\nlibrary(ggplot2)\n\ndf &lt;- data.frame( Val=NA, BioLayer=NA, Category=NA )\n\nlayers &lt;- names(vals.pred)\nfor( layer in layers){\n  Val &lt;- c( vals.pred[[layer]], vals.rand[[layer]] )\n  Category &lt;- c( rep(\"Observed\",nrow(vals.pred)), rep(\"Background\",nrow(vals.rand)))\n  df &lt;- rbind( df, data.frame( Val, BioLayer=layer, Category))\n}\ndf$BioLayer &lt;- factor( df$BioLayer, ordered=TRUE, \n                       levels = names(vals.pred)[c(1,2,5,6,3,4)])\n                                                                              \n\ndf$Category &lt;- factor( df$Category )\ndf &lt;- df[ !is.na(df$Val),]\np &lt;- ggplot(df,aes(x=Val, fill=Category)) + geom_density(alpha=0.75)  \np &lt;- p + facet_wrap(~BioLayer, nrow=3, scale=\"free\") \np &lt;- p + scale_fill_brewer(type=\"qual\",palette=3) \np + xlab(\"Biolayer Value\") + ylab(\"Density\")",
    "crumbs": [
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Niche Modeling</span>"
    ]
  },
  {
    "objectID": "data_sets.html",
    "href": "data_sets.html",
    "title": "\n37  Included Data Sets\n",
    "section": "",
    "text": "37.1 The Sonoran desert bark beetle, Araptus attenuata\nThe content of this book includes several data sets that come with the gstudio and popgraph libraries. Here is a brief overview of those data sets with references to the manuscripts from which they were analyzed.\nknitr::include_graphics(\"media/araptus.png\")\nArapatus attenuatus is a bark beetle endemic to the Sonoran desert. It is known only from the senescing stems of the desert plant, Euphorbia lomelii (syn Pedilanthus macrocarpus). Samples for this data set were collected from 39 populations throughout the species range.\nlibrary(ggplot2)\nlibrary(ggmap)\nlibrary(gstudio)\nlibrary(ggrepel)\ndata(arapat)\ncoords &lt;- strata_coordinates(arapat)\nmap &lt;- population_map(coords)\nggmap(map) + geom_point(aes(x=Longitude,y=Latitude), data=coords) + xlab(\"Longitude\") + ylab(\"Latitude\") + geom_text_repel(aes(x=Longitude, y=Latitude, label=Stratum), data=coords)",
    "crumbs": [
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Included Data Sets</span>"
    ]
  },
  {
    "objectID": "data_sets.html#the-sonoran-desert-bark-beetle-araptus-attenuata",
    "href": "data_sets.html#the-sonoran-desert-bark-beetle-araptus-attenuata",
    "title": "\n37  Included Data Sets\n",
    "section": "",
    "text": "37.1.1 Nuclear Markers\nThe markers in the arapat data set consist of eight polymorphic and codominant nuclear markers.\n\nlibrary(gstudio)\ndata(arapat)\ncolumn_class( arapat, \"locus\")\n\n[1] \"LTRS\" \"WNT\"  \"EN\"   \"EF\"   \"ZMP\"  \"AML\"  \"ATPS\" \"MP20\"\n\n\nThese markers span a range of allelic diversity and richness.\n\ndf &lt;- merge( genetic_diversity(arapat,mode=\"A\"), genetic_diversity(arapat,mode=\"He\"))\ndf &lt;- merge( df, genetic_diversity(arapat, mode=\"Ae\"))\nknitr::kable(df,align=c(\"l\",\"c\",\"c\",\"c\"),digits=3)\n\n\n\nLocus\nA\nHe\nAe\n\n\n\nAML\n13\n0.829\n5.861\n\n\nATPS\n10\n0.719\n3.563\n\n\nEF\n2\n0.436\n1.774\n\n\nEN\n5\n0.449\n1.815\n\n\nLTRS\n2\n0.499\n1.996\n\n\nMP20\n19\n0.819\n5.512\n\n\nWNT\n5\n0.653\n2.880\n\n\nZMP\n2\n0.339\n1.514\n\n\n\n\n\n\n37.1.2 Methylation Markers\nIn addition to codominant nuclear markers, an msAFLP analysis was performed on the major clade and paired sequence and \\(C_pG\\) methylation markers were derived.",
    "crumbs": [
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Included Data Sets</span>"
    ]
  },
  {
    "objectID": "data_sets.html#the-flowering-dogwood-cornus-florida",
    "href": "data_sets.html#the-flowering-dogwood-cornus-florida",
    "title": "\n37  Included Data Sets\n",
    "section": "\n37.2 The Flowering Dogwood Cornus florida\n",
    "text": "37.2 The Flowering Dogwood Cornus florida\n\n\nknitr::include_graphics(\"media/dogwood_flower.png\",)\n\n\n\n\n\n\nFigure 37.1: Inflorescence for flowering dogwood with conspicuous showy bracts and many small flowers in the center.\n\n\n\n\nFlowering dogwood is an ubiquitous feature of eastern north american forests.\nThe data set contains five microsatellite loci.\n\ndata(cornus)\ncolumn_class( cornus, \"locus\")\n\n[1] \"Cf.G8\"  \"Cf.H18\" \"Cf.N5\"  \"Cf.N10\" \"Cf.O5\" \n\n\nof roughly equal genetic diversity.\n\ndf &lt;- merge( genetic_diversity(cornus,mode=\"A\"), genetic_diversity(cornus,mode=\"He\"))\ndf &lt;- merge( df, genetic_diversity(cornus, mode=\"Ae\"))\nknitr::kable(df,align=c(\"l\",\"c\",\"c\",\"c\"),digits=3)\n\n\n\nLocus\nA\nHe\nAe\n\n\n\nCf.G8\n28\n0.931\n14.482\n\n\nCf.H18\n20\n0.925\n13.325\n\n\nCf.N10\n18\n0.903\n10.349\n\n\nCf.N5\n11\n0.453\n1.827\n\n\nCf.O5\n15\n0.860\n7.146",
    "crumbs": [
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Included Data Sets</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Population Genetics",
    "section": "",
    "text": "Applied Population Genetics\nknitr::include_graphics(\"media/APG-Title-Wide.png\")\n\n\n\n\n\n\nFigure 1\nBuild Date: Mon Jan 5 15:58:44 2026\nThe content of this text is modified, added to, and changed continually. You are welcome to use the content of this book in its present form and you can provide feedback, comments, or suggestions for additions by contacting me at rjdyer@vcu.edu. This work will continue to be hosted online and continually updated as new approaches and applications become available.\nlibrary(ggplot2)\nlibrary(knitr)\ntheme_set( theme_bw(base_size=14) )\nknitr::opts_chunk$set( warning=FALSE, message=FALSE,error = FALSE )",
    "crumbs": [
      "Applied Population Genetics"
    ]
  },
  {
    "objectID": "index.html#logistics",
    "href": "index.html#logistics",
    "title": "Applied Population Genetics",
    "section": "Logistics",
    "text": "Logistics\n© 2017 by R.J. Dyer.\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. Under this license, authorized individuals may copy, distribute, display and perform the work and make derivative works and remixes based on this text only if they give the original author credit (attribution). You are also free to distribute derivative works only under a license identical (“not more restrictive”) to the license that governs this original work.\nDr. Rodney Dyer is an Associate Professor and Director for the Center for Environmental Studies at Virginia Commonwealth University in Richmond, Virginia, USA. His research focuses on genetic connectivity and structure and how the environment influences both. More information on his research can be found at http://dyerlab.org.",
    "crumbs": [
      "Applied Population Genetics"
    ]
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Applied Population Genetics",
    "section": "Motivation",
    "text": "Motivation\nThis is, most of all, not a book about R. This is also not a “Population Genetics in R” textbook. It is a book about how we do population genetic analyses, for which R is a tool that allows us to reach beyond the limitations of point-and-click interfaces. As a field, Population Genetics has a broad set of textbooks describing the underlying theory. As a student, I cut my teeth on the texts of Hartl (1981), Hartl & Clark (1997) and have used other great texts such as Hamilton (2011) and Hedrick (2009) in the classroom to teach the subject for the last decade. In late 2015, there are a host of texts available to the student of population genetics—amazon lists 150 different books under the search term “Population Genetics textbook”—why do another one? What I have found is that while the theory behind this discipline has been well developed, its application has been largely neglected.\nAs a new graduate student, fresh out of my first population genetics course, I felt armed with the understanding of how microevolutionary processes influence the distribution of alleles within and among populations. What I wasn’t prepared for was sitting in front of the computer, looking at a few thousand individuals assayed for several different loci and actually ‘doing’ population genetics. All of those textbooks provide me with what is expected and the theory behind it, though often fall short on teaching me how I could apply those inferences to data I actually collect. If you are a theoretical population geneticist, those texts and your ability to integrate mathematical equations will provide you a research lifetime of work. However, if you are practitioner who uses population genetic tools to answer conservation, management, or ecologically inspired questions, the evolutionary expectations of population genetic processes will most likely not be as important as directly estimating inbreeding, exploring ongoing connectivity, or determining genetic granularity of existing populations. This is where this textbook is focusing, a seemingly uninhabited niche in the knowledge ecosystem of graduate level population genetics.\nThis text was developed out of a graduate course in Population Genetics that I’ve been teaching at Virginia Commonwealth University since 2005. This texts uses R and many additional libraries available within the R ecosystem to illustrate how to perform specific types of analyses and what kind of biological inferences we can gain from them. In the process, we cover materials that are commonly needed in the application of population genetic analysis such as spatial autocorrelation, paternity analysis, and the use of permutation while at the same time highlighting logistical challenges commonly encountered in analyzing real data such as incomplete sampling, missing data, and rarefaction.",
    "crumbs": [
      "Applied Population Genetics"
    ]
  },
  {
    "objectID": "index.html#typography",
    "href": "index.html#typography",
    "title": "Applied Population Genetics",
    "section": "Typography",
    "text": "Typography\nThis text is designed primarily as an electronic publication (ePub) and has dynamical content included within. If you are reading a static copy (as PDF or printed text), you are not getting the full experience of what this book has been designed to deliver. Much of the text is devoted to quantitative analysis of data in R, and as such I’ve typeset the R components differently from the flowing text. Text intended to be input into R is typeset as a fixed width font and colored using the default color scheme found in RStudio (http://rstudio.org). Here are two input examples.\n\nvalue &lt;- 20\nname &lt;- \"Perdedor\"\n\nThis text is amenable to copy-and-paste action so you can perform the same calculations on your computer as are done in the text. When R returns an answer to an analysis or prints the contents of a variable out, the results are also typeset in fixed width font but each line is prefixed by two hash marks.\n\nrnorm(10)\n\n [1]  0.2769213  0.2122282  0.4786492  0.7648880 -0.1164361 -0.5372516\n [7]  0.1074814  1.8414440 -1.2771785 -0.7950362\n\n\nThe lines with hashes are not something you are going to copy-and-paste into your R session, it is what R is giving you. Inline code (e.g., code inserted into a sentence in a descriptive context such as discussing the rnorm() function in the previous example) is typeset similarly though not necessarily intended to be cut-and-pasted into your session.\nThroughout the ePub, there are also dynamical content. Some of this content may be condensed text put into a scrolling text-box. Here the notion is to provide direct access to information without unnecessarily adding length to the overall text. Here is an example, showing the documentation associated with the R help.start() function within a scrolling text box.\n\nhelp.start {utils}                                              R Documentation\n\nHypertext Documentation\n\nDescription\n\nStart the hypertext (currently HTML) version of R's online documentation.\n\nUsage\n\nhelp.start(update = FALSE, gui = \"irrelevant\", browser = getOption(\"browser\"), remote = NULL)\n\nArguments\n\nupdate - logical: should this attempt to update the package index to reflect the currently available packages. (Not attempted if remote is non-NULL.)\ngui - just for compatibility with S-PLUS.\nbrowser - the name of the program to be used as hypertext browser. It should be in the PATH, or a full path specified. Alternatively, it can be an R function which will be called with a URL as its only argument. This option is normally unset on Windows, when the file-association mechanism will be used.\nremote - A character string giving a valid URL for the 'R_HOME' directory on a remote location.\n\nDetails\n\nUnless remote is specified this requires the HTTP server to be available (it will be started if possible: see startDynamicHelp).\n\nOne of the links on the index page is the HTML package index, 'R.home(\"docs\")/html/packages.html', which can be remade by make.packages.html(). For local operation, the HTTP server will remake a temporary version of this list when the link is first clicked, and each time thereafter check if updating is needed (if .libPaths has changed or any of the directories has been changed). This can be slow, and using update = TRUE will ensure that the packages list is updated before launching the index page.\n\nArgument remote can be used to point to HTML help published by another R installation: it will typically only show packages from the main library of that installation.\n\nSee Also\n\nhelp() for on- and off-line help in other formats.\nbrowseURL for how the help file is displayed.\nRSiteSearch to access an on-line search of R resources.\n\nExamples\n\nhelp.start()\n## Not run:\n## the 'remote' arg can be tested by\nhelp.start(remote = paste0(\"file://\", R.home()))\n\n## End(Not run)\n\n\nIn addition to shoving static content into scrolling windows, longer R scripts will also be inserted into these widgets so that space can be saved while preserving syntax highlighting and code format.",
    "crumbs": [
      "Applied Population Genetics"
    ]
  },
  {
    "objectID": "index.html#interactive-content",
    "href": "index.html#interactive-content",
    "title": "Applied Population Genetics",
    "section": "Interactive Content",
    "text": "Interactive Content\nWhere possible, I have included interactive content in the text. Examples include dynamical plots such as embedding google map objects in the page, network structure that can be manipulated in the browser, and widgets that show how a process influences popualtion genetic features by allowing you to direclty manipulate the parameters in the model.",
    "crumbs": [
      "Applied Population Genetics"
    ]
  },
  {
    "objectID": "index.html#dedication",
    "href": "index.html#dedication",
    "title": "Applied Population Genetics",
    "section": "Dedication",
    "text": "Dedication\nI would like to dedicate this text and the motivations and inspirations that underly its creation, to my thesis advisor, Dr. Victoria L. Sork. She has an uncanny ability to see beyond the questions that we know how to answer and to help us focus on the answers that we are really trying to find.",
    "crumbs": [
      "Applied Population Genetics"
    ]
  }
]